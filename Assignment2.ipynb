{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: %s\" % device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Arguments\n",
    "arguments_training_url = (\n",
    "    \"https://zenodo.org/records/8248658/files/arguments-training.tsv?download=1\"\n",
    ")\n",
    "arguments_validation_url = (\n",
    "    \"https://zenodo.org/records/8248658/files/arguments-validation.tsv?download=1\"\n",
    ")\n",
    "arguments_test_url = (\n",
    "    \"https://zenodo.org/records/8248658/files/arguments-test.tsv?download=1\"\n",
    ")\n",
    "\n",
    "### Human values\n",
    "labels_training_url = (\n",
    "    \"https://zenodo.org/records/8248658/files/labels-training.tsv?download=1\"\n",
    ")\n",
    "labels_validation_url = (\n",
    "    \"https://zenodo.org/records/8248658/files/labels-validation.tsv?download=1\"\n",
    ")\n",
    "labels_test_url = \"https://zenodo.org/records/8248658/files/labels-test.tsv?download=1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating Training dataframe\n",
    "arguments_tr_df = pd.read_csv(arguments_training_url, sep=\"\\t\")\n",
    "labels_tr_df = pd.read_csv(labels_training_url, sep=\"\\t\")\n",
    "\n",
    "### Merging arguments and labels\n",
    "train_df_nm = pd.merge(arguments_tr_df, labels_tr_df, on=\"Argument ID\")\n",
    "\n",
    "### Creating Validation dataframe\n",
    "arguments_va_df = pd.read_csv(arguments_validation_url, sep=\"\\t\")\n",
    "labels_va_df = pd.read_csv(labels_validation_url, sep=\"\\t\")\n",
    "\n",
    "### Merging arguments and labels\n",
    "validation_df_nm = pd.merge(arguments_va_df, labels_va_df, on=\"Argument ID\")\n",
    "\n",
    "### Creating Test dataframe\n",
    "arguments_te_df = pd.read_csv(arguments_test_url, sep=\"\\t\")\n",
    "labels_te_df = pd.read_csv(labels_test_url, sep=\"\\t\")\n",
    "\n",
    "### Merging arguments and labels\n",
    "test_df_nm = pd.merge(arguments_te_df, labels_te_df, on=\"Argument ID\")\n",
    "\n",
    "### Notation\n",
    "### nm=not merged with logical OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_nm.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_nm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Considering category ranges (0,3),(3,7),(7,13),(13,19)\n",
    "### adding +4, considering the first 4 columns which are not categories\n",
    "column_ranges = [(4, 7), (7, 11), (11, 17), (17, 23)]\n",
    "level_3_cat = [\n",
    "    \"Openness_to_change\",\n",
    "    \"Self_enhancement\",\n",
    "    \"Conversation\",\n",
    "    \"Self_transcendence\",\n",
    "]\n",
    "columns_to_keep = [\"Argument ID\", \"Conclusion\", \"Stance\", \"Premise\"]\n",
    "\n",
    "### Creating final dataframes\n",
    "train_df = pd.DataFrame()\n",
    "validation_df = pd.DataFrame()\n",
    "test_df = pd.DataFrame()\n",
    "\n",
    "### Applying OR to the selected columns using .any(axis=1)\n",
    "for (start, end), cat in zip(column_ranges, level_3_cat):\n",
    "    train_df[cat] = train_df_nm.iloc[:, start:end].any(axis=1)\n",
    "    validation_df[cat] = validation_df_nm.iloc[:, start:end].any(axis=1)\n",
    "    test_df[cat] = test_df_nm.iloc[:, start:end].any(axis=1)\n",
    "\n",
    "### Reading the columns to keep\n",
    "train_df = pd.concat([train_df_nm[columns_to_keep], train_df], axis=1)\n",
    "validation_df = pd.concat([validation_df_nm[columns_to_keep], validation_df], axis=1)\n",
    "test_df = pd.concat([test_df_nm[columns_to_keep], test_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"Openness_to_change\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define a mapping for \"Stance\" column\n",
    "stance_mapping = {\"in favor of\": True, \"against\": False}\n",
    "\n",
    "### Apply the mapping to convert strings to boolean values\n",
    "train_df[\"Stance\"] = train_df[\"Stance\"].map(stance_mapping)\n",
    "validation_df[\"Stance\"] = validation_df[\"Stance\"].map(stance_mapping)\n",
    "test_df[\"Stance\"] = test_df[\"Stance\"].map(stance_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345678)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_list = [DummyClassifier(strategy=\"uniform\") for _ in level_3_cat]\n",
    "[\n",
    "    clf.fit(X=train_df[columns_to_keep[1:]], y=train_df[cat])\n",
    "    for clf, cat in zip(clf_list, level_3_cat)\n",
    "]\n",
    "prediction_uniform = np.array(\n",
    "    [clf.predict(X=test_df[columns_to_keep[1:]]) for clf in clf_list]\n",
    ").T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 score for Unifrom Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluate F1 overall\n",
    "f1_overall = f1_score(\n",
    "    y_true=test_df[level_3_cat], y_pred=prediction_uniform, average=\"weighted\"\n",
    ")\n",
    "print(f\"Random Classifier F1 overall weighted : {f1_overall:.4f}\")\n",
    "\n",
    "### Evaluate F1 overall\n",
    "f1_overall = f1_score(\n",
    "    y_true=test_df[level_3_cat], y_pred=prediction_uniform, average=\"macro\"\n",
    ")\n",
    "print(f\"Random Classifier F1 overall macro: {f1_overall:.4f}\")\n",
    "\n",
    "\n",
    "### Evaluate F1 per category\n",
    "f1_per_cat = [\n",
    "    f1_score(y_true=test_df[cat], y_pred=prediction_uniform[:, i])\n",
    "    for i, cat in enumerate(level_3_cat)\n",
    "]\n",
    "print(f\"Random Classifier F1 per category: {f1_per_cat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Majority Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_list = [DummyClassifier(strategy=\"most_frequent\") for _ in level_3_cat]\n",
    "[\n",
    "    clf.fit(X=train_df[columns_to_keep[1:]], y=train_df[cat])\n",
    "    for clf, cat in zip(clf_list, level_3_cat)\n",
    "]\n",
    "prediction_majority = np.array(\n",
    "    [clf.predict(X=test_df[columns_to_keep[1:]]) for clf in clf_list]\n",
    ").T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 score for Majority Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluate over all F1\n",
    "f1_overall = f1_score(\n",
    "    y_true=test_df[level_3_cat], y_pred=prediction_majority, average=\"weighted\"\n",
    ")\n",
    "print(f\"Majority Classifier F1 weighted : {f1_overall:.4f}\")\n",
    "\n",
    "### Evaluate over all F1\n",
    "f1_overall = f1_score(\n",
    "    y_true=test_df[level_3_cat], y_pred=prediction_majority, average=\"macro\"\n",
    ")\n",
    "print(f\"Majority Classifier F1 macro: {f1_overall:.4f}\")\n",
    "\n",
    "\n",
    "### Evaluate F1 per category\n",
    "f1_per_cat = [\n",
    "    f1_score(y_true=test_df[cat], y_pred=prediction_majority[:, i])\n",
    "    for i, cat in enumerate(level_3_cat)\n",
    "]\n",
    "print(f\"Random Classifier F1 per category: {f1_per_cat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert dataframes into datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "validation_dataset = Dataset.from_pandas(validation_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {idx: label for idx, label in enumerate(level_3_cat)}\n",
    "label2id = {label: idx for idx, label in enumerate(level_3_cat)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_card = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_card)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_card,\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    num_labels=len(level_3_cat),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion Only Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Encoding for Conclusion only model\n",
    "def tokenize_conclusion(example):\n",
    "    ### Tokenize text columns\n",
    "    text_tokens = tokenizer(\n",
    "        example[\"Conclusion\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    ### Combine text tokens with non-text features\n",
    "    encoded_example = {\n",
    "        \"input_ids\": text_tokens[\"input_ids\"],\n",
    "        \"token_type_ids\": text_tokens[\"token_type_ids\"],\n",
    "        \"attention_mask\": text_tokens[\"attention_mask\"],\n",
    "        \"Openness_to_change\": torch.tensor(\n",
    "            example[\"Openness_to_change\"], dtype=torch.float\n",
    "        ),\n",
    "        \"Self_enhancement\": torch.tensor(\n",
    "            example[\"Self_enhancement\"], dtype=torch.float\n",
    "        ),\n",
    "        \"Conversation\": torch.tensor(example[\"Conversation\"], dtype=torch.float),\n",
    "        \"Self_transcendence\": torch.tensor(\n",
    "            example[\"Self_transcendence\"], dtype=torch.float\n",
    "        ),\n",
    "    }\n",
    "    # print(encoded_example)\n",
    "    return encoded_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenize train, validation, test datasets\n",
    "ds_list = [\n",
    "    d.map(tokenize_conclusion, batched=True)\n",
    "    for d in (train_dataset, validation_dataset, test_dataset)\n",
    "]\n",
    "\n",
    "### Set format for train, validation, test tokenized datasets\n",
    "columns = [\n",
    "    \"input_ids\",\n",
    "    \"token_type_ids\",\n",
    "    \"attention_mask\",\n",
    "    \"Openness_to_change\",\n",
    "    \"Self_enhancement\",\n",
    "    \"Conversation\",\n",
    "    \"Self_transcendence\",\n",
    "]\n",
    "\n",
    "for d in ds_list:\n",
    "    d.set_format(type=\"torch\", columns=columns)\n",
    "train_tokenized_ds, valid_tokenized_ds, test_tokenized_ds = ds_list\n",
    "\n",
    "# ### Tokenize training data\n",
    "# test_tokenized_dataset = test_dataset.map(tokenize_conclusion, batched=True)\n",
    "# test_tokenized_dataset.set_format(\n",
    "#     \"tensorflow\",\n",
    "#     columns=[\n",
    "#         \"input_ids\",\n",
    "#         \"token_type_ids\",\n",
    "#         \"attention_mask\",\n",
    "#         \"Openness_to_change\",\n",
    "#         \"Self_enhancement\",\n",
    "#         \"Conversation\",\n",
    "#         \"Self_transcendence\",\n",
    "#     ],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sanity check\n",
    "print(train_tokenized_ds[\"Conclusion\"][50])\n",
    "decoded_text = tokenizer.decode(train_tokenized_ds[\"input_ids\"][50])\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_card, num_labels=len(level_3_cat), id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    metrics = f1_score(y_true=y_true, y_pred=y_pred, average=\"macro\")\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def compute_metrics(prediction):\n",
    "    preds = (\n",
    "        prediction.predictions[0]\n",
    "        if isinstance(prediction.predictions, tuple)\n",
    "        else prediction.predictions\n",
    "    )\n",
    "    result = multi_label_metrics(predictions=preds, labels=prediction.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/Models/BertBaseUncased\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    "    save_strategy=\"epoch\",  #'no'\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=1,  ### fine tuning\n",
    "    weight_decay=0.01,\n",
    "    metric_for_best_model=\"f1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized_ds,\n",
    "    eval_dataset=valid_tokenized_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion - Premise Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Encoding for Conclusion - Premise model\n",
    "def tokenize_conclusion_premise(example):\n",
    "    ### Tokenize text columns\n",
    "    text_tokens = tokenizer(\n",
    "        example[\"Conclusion\"],\n",
    "        example[\"Premise\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    ### Combine text tokens with non-text features\n",
    "    encoded_example = {\n",
    "        \"input_ids\": text_tokens[\"input_ids\"],\n",
    "        \"token_type_ids\": text_tokens[\"token_type_ids\"],\n",
    "        \"attention_mask\": text_tokens[\"attention_mask\"],\n",
    "        \"Openness_to_change\": torch.tensor(\n",
    "            example[\"Openness_to_change\"], dtype=torch.bool\n",
    "        ),\n",
    "        \"Self_enhancement\": torch.tensor(example[\"Self_enhancement\"], dtype=torch.bool),\n",
    "        \"Conversation\": torch.tensor(example[\"Conversation\"], dtype=torch.bool),\n",
    "        \"Self_transcendence\": torch.tensor(\n",
    "            example[\"Self_transcendence\"], dtype=torch.bool\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    return encoded_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion - Premise - Stance Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Encoding for Conclusion - Premise - Stance model\n",
    "def tokenize_conclusion_premise_stance(example):\n",
    "    ### Tokenize text columns\n",
    "    text_tokens = tokenizer(\n",
    "        example[\"Conclusion\"],\n",
    "        example[\"Premise\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    ### Combine text tokens with non-text features\n",
    "    encoded_example = {\n",
    "        \"input_ids\": text_tokens[\"input_ids\"],\n",
    "        \"token_type_ids\": text_tokens[\"token_type_ids\"],\n",
    "        \"attention_mask\": text_tokens[\"attention_mask\"],\n",
    "        \"Stance\": torch.tensor(\n",
    "            example[\"Stance\"], dtype=torch.bool\n",
    "        ),  ### Assuming 'Stance' is represented as 0 or 1\n",
    "        \"Openness_to_change\": torch.tensor(\n",
    "            example[\"Openness_to_change\"], dtype=torch.bool\n",
    "        ),\n",
    "        \"Self_enhancement\": torch.tensor(example[\"Self_enhancement\"], dtype=torch.bool),\n",
    "        \"Conversation\": torch.tensor(example[\"Conversation\"], dtype=torch.bool),\n",
    "        \"Self_transcendence\": torch.tensor(\n",
    "            example[\"Self_transcendence\"], dtype=torch.bool\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    return encoded_example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
