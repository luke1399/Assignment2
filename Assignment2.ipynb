{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBky0WRKnLDP"
      },
      "outputs": [],
      "source": [
        "%pip install datasets -q\n",
        "%pip install accelerate -U -q\n",
        "%pip install transformers -q\n",
        "%pip install scikit-learn==1.3.2 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLQEVUNkmupt"
      },
      "outputs": [],
      "source": [
        "import urllib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "from transformers import (\n",
        "    BertModel,\n",
        "    BertTokenizer,\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm.notebook as tq\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LITvRFBXmupv"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device: %s\" % device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XD5WRYKmupv"
      },
      "source": [
        "## TASK 1: Corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3HAVDQ9mupw"
      },
      "outputs": [],
      "source": [
        "### Argument urls\n",
        "argument_urls = {\n",
        "    \"train\": \"https://zenodo.org/records/8248658/files/arguments-training.tsv?download=1\",\n",
        "    \"validation\": \"https://zenodo.org/records/8248658/files/arguments-validation.tsv?download=1\",\n",
        "    \"test\": \"https://zenodo.org/records/8248658/files/arguments-test.tsv?download=1\",\n",
        "}\n",
        "\n",
        "### Human values urls\n",
        "level2_values_urls = {\n",
        "    \"train\": \"https://zenodo.org/records/8248658/files/labels-training.tsv?download=1\",\n",
        "    \"validation\": \"https://zenodo.org/records/8248658/files/labels-validation.tsv?download=1\",\n",
        "    \"test\": \"https://zenodo.org/records/8248658/files/labels-test.tsv?download=1\",\n",
        "}\n",
        "### Check that the splits in the dict keys were not misspelled\n",
        "for split in level2_values_urls.keys():\n",
        "    assert split in argument_urls.keys(), \"url dictionary keys misspelled\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbPdz0AMyjRZ"
      },
      "outputs": [],
      "source": [
        "### Create Data folder\n",
        "data_folder = Path.cwd().joinpath(\"Data\")\n",
        "if not data_folder.exists():\n",
        "    data_folder.mkdir(parents=True)\n",
        "\n",
        "### Path for each file\n",
        "argument_paths = {\n",
        "    split: data_folder.joinpath(f\"arguments_{split}.tsv\")\n",
        "    for split in argument_urls.keys()\n",
        "}\n",
        "level2_values_paths = {\n",
        "    split: data_folder.joinpath(f\"labels_{split}.tsv\")\n",
        "    for split in level2_values_urls.keys()\n",
        "}\n",
        "\n",
        "### Download each file if it's not already there\n",
        "for file, path in argument_paths.items():\n",
        "    if not path.exists():\n",
        "        urllib.request.urlretrieve(argument_urls[file], filename=path)\n",
        "for file, path in level2_values_paths.items():\n",
        "    if not path.exists():\n",
        "        urllib.request.urlretrieve(level2_values_urls[file], filename=path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtSpCbncmupw"
      },
      "outputs": [],
      "source": [
        "### Read arguments file (split = train/validation/test)\n",
        "argument_dfs = {\n",
        "    split: pd.read_csv(path, sep=\"\\t\") for split, path in argument_paths.items()\n",
        "}\n",
        "\n",
        "### Read human values file (split = train/validation/test)\n",
        "level2_values_dfs = {\n",
        "    values_split: pd.read_csv(path, sep=\"\\t\")\n",
        "    for values_split, path in level2_values_paths.items()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJDNc3hQyjRb"
      },
      "outputs": [],
      "source": [
        "### Merge arguments and (labels) level 2 values (split = train/validation/test)\n",
        "args_level2vals_dfs = {\n",
        "    split: pd.merge(argument, level2_values_dfs[split], on=\"Argument ID\")\n",
        "    for split, argument in argument_dfs.items()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVuY57e6yjRd"
      },
      "outputs": [],
      "source": [
        "level2_values_dfs[\"train\"].head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvRAFeEiesCz"
      },
      "outputs": [],
      "source": [
        "argument_dfs[\"train\"].head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FJvn74Bmupw"
      },
      "outputs": [],
      "source": [
        "args_level2vals_dfs[\"train\"].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0z1YWKtAmupw"
      },
      "outputs": [],
      "source": [
        "args_level2vals_dfs[\"train\"].head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txRosokSyjRf"
      },
      "outputs": [],
      "source": [
        "### Considering category ranges (0,3),(3,7),(7,13),(13,19)\n",
        "### adding +4, considering the first 4 columns which are not categories\n",
        "level3_categories_ranges = {\n",
        "    \"Openness_to_change\": (4, 7),\n",
        "    \"Self_enhancement\": (7, 11),\n",
        "    \"Conversation\": (11, 17),\n",
        "    \"Self_transcendence\": (17, 23),\n",
        "}\n",
        "columns_to_keep = [\"Argument ID\", \"Conclusion\", \"Stance\", \"Premise\"]\n",
        "level_3_cat = list(level3_categories_ranges.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQ0eequuyjRg"
      },
      "outputs": [],
      "source": [
        "### Creating final dataframes\n",
        "train, validation, test = args_level2vals_dfs.keys()\n",
        "assert train == \"train\" and validation == \"validation\" and test == \"test\"\n",
        "\n",
        "### nm = not merged\n",
        "train_df_nm = args_level2vals_dfs[\"train\"]\n",
        "validation_df_nm = args_level2vals_dfs[\"validation\"]\n",
        "test_df_nm = args_level2vals_dfs[\"test\"]\n",
        "\n",
        "### Creating final dataframes\n",
        "train_df = pd.DataFrame()\n",
        "validation_df = pd.DataFrame()\n",
        "test_df = pd.DataFrame()\n",
        "\n",
        "### Merge lvl2 to lvl 3 (any = OR)\n",
        "for cat, (start, end) in level3_categories_ranges.items():\n",
        "    train_df[cat] = train_df_nm.iloc[:, start:end].any(axis=1)\n",
        "    validation_df[cat] = validation_df_nm.iloc[:, start:end].any(axis=1)\n",
        "    test_df[cat] = test_df_nm.iloc[:, start:end].any(axis=1)\n",
        "\n",
        "### Adding the columns to keep of the original dfs\n",
        "train_df = pd.concat([train_df_nm[columns_to_keep], train_df], axis=1)\n",
        "validation_df = pd.concat([validation_df_nm[columns_to_keep], validation_df], axis=1)\n",
        "test_df = pd.concat([test_df_nm[columns_to_keep], test_df], axis=1)\n",
        "\n",
        "### Define a mapping for \"Stance\" column\n",
        "stance_mapping = {\"in favor of\": 1, \"against\": 0}\n",
        "\n",
        "### Apply the mapping to convert strings to boolean values\n",
        "train_df[\"Stance\"] = train_df[\"Stance\"].map(stance_mapping)\n",
        "validation_df[\"Stance\"] = validation_df[\"Stance\"].map(stance_mapping)\n",
        "test_df[\"Stance\"] = test_df[\"Stance\"].map(stance_mapping)\n",
        "\n",
        "dfs = {\"train\": train_df, \"validation\": validation_df, \"test\": test_df}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVahwSKQyjRg"
      },
      "outputs": [],
      "source": [
        "train_df[\"Conversation\"].sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5NsJaBIesC2"
      },
      "outputs": [],
      "source": [
        "train_df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbzBp0BLesC2"
      },
      "outputs": [],
      "source": [
        "train_df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdC_EHS0mupx"
      },
      "source": [
        "# TASK 2: Model definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xtHCuBamupx"
      },
      "source": [
        "## Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1nlQvZ9yjRh"
      },
      "outputs": [],
      "source": [
        "def baseline_model(strategy, level_3_cat, train_df, columns_to_keep, seed=None):\n",
        "    clf_list = [\n",
        "        DummyClassifier(strategy=strategy, random_state=seed) for _ in level_3_cat\n",
        "    ]\n",
        "    [\n",
        "        clf.fit(X=train_df[columns_to_keep[1:]], y=train_df[cat])\n",
        "        for clf, cat in zip(clf_list, level_3_cat)\n",
        "    ]\n",
        "    return clf_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3dtkYMRmupy"
      },
      "source": [
        "## Bert - base Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1Y_4AZ2mupy"
      },
      "outputs": [],
      "source": [
        "### Convert dataframes into datasets\n",
        "datasets = {split: Dataset.from_pandas(df) for split, df in dfs.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fl7AnlkByjRh"
      },
      "outputs": [],
      "source": [
        "def compute_class_weights(df, cat_labels):\n",
        "    labels_array = df[cat_labels].to_numpy()\n",
        "    n_ones = np.sum(labels_array, axis=0, dtype=np.single)\n",
        "    weights = np.empty_like(n_ones)\n",
        "    n_zeroes = np.array([labels_array.shape[0] - o for o in n_ones])\n",
        "\n",
        "    for class_num, (ones, zeroes) in enumerate(zip(n_ones, n_zeroes)):\n",
        "        weights[class_num] = zeroes / (ones + 1e-4)\n",
        "\n",
        "    print(f\"weigts = {weights}\")\n",
        "    return torch.as_tensor(weights, dtype=torch.float).to(device)\n",
        "\n",
        "\n",
        "def compute_class_weights_root(df, cat_labels):\n",
        "    labels_array = df[cat_labels].to_numpy()\n",
        "    n_ones = np.sum(labels_array, axis=0, dtype=np.single)\n",
        "    weights = np.empty_like(n_ones)\n",
        "    n_zeroes = np.array([labels_array.shape[0] - o for o in n_ones])\n",
        "\n",
        "    for class_num, ones in enumerate(n_ones):\n",
        "        weights[class_num] = np.sqrt(labels_array.shape[0] / (ones + 1e-4))\n",
        "\n",
        "    print(f\"weigts = {weights}\")\n",
        "    return torch.as_tensor(weights, dtype=torch.float).to(device)\n",
        "\n",
        "\n",
        "def loss_fn(outputs, targets, pos_weight=None):\n",
        "    return torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)(outputs, targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zgsc-qEeyjRi"
      },
      "outputs": [],
      "source": [
        "def add_labels(ds_row, labels):\n",
        "    labels_batch = {k: ds_row[k] for k in ds_row.keys() if k in labels}\n",
        "    labels_matrix = np.zeros((len(ds_row[\"Conclusion\"]), len(labels)))\n",
        "    for i, label in enumerate(labels):\n",
        "        labels_matrix[:, i] = labels_batch[label]\n",
        "    return labels_matrix.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmNPnBnstg4K"
      },
      "source": [
        "### Bert-base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVg7yRcWtg4K"
      },
      "outputs": [],
      "source": [
        "class BERTClass(torch.nn.Module):\n",
        "    def __init__(self, stance=False):\n",
        "        super().__init__()\n",
        "        self.bert_model = BertModel.from_pretrained(\n",
        "            \"bert-base-uncased\", return_dict=True\n",
        "        )\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        if not stance:\n",
        "            self.linear = torch.nn.Linear(768, len(level_3_cat))\n",
        "        else:\n",
        "            ### 769! there is \"stance\" as another input\n",
        "            self.linear = torch.nn.Linear(769, len(level_3_cat))\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids, attn_mask, stance=None):\n",
        "        output = self.bert_model(\n",
        "            input_ids, attention_mask=attn_mask, token_type_ids=token_type_ids\n",
        "        )\n",
        "        output_dropout = self.dropout(output.pooler_output)\n",
        "\n",
        "        if stance is None:\n",
        "            output_linear = self.linear(output_dropout)\n",
        "        else:\n",
        "            ### concatenate stance\n",
        "            stance = stance.view(stance.shape[0], -1)\n",
        "            output_stack = torch.cat((output_dropout, stance), dim=1)\n",
        "            output_linear = self.linear(output_stack)\n",
        "        return output_linear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KC8IMAftg4K"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_a-V-56otg4K"
      },
      "outputs": [],
      "source": [
        "### Encoding\n",
        "def tokenize(ds_row, tokenizer=tokenizer, premise=False, stance=False):\n",
        "    ### Tokenize text columns\n",
        "    print(f\"PREMISE = {premise}  STANCE = {stance}\")\n",
        "    if not premise:\n",
        "        text_tokens = tokenizer(\n",
        "            ds_row[\"Conclusion\"],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=tokenizer.model_max_length // 2,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "    else:\n",
        "        text_tokens = tokenizer(\n",
        "            ds_row[\"Conclusion\"],\n",
        "            ds_row[\"Premise\"],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=tokenizer.model_max_length // 2,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "    ### Combine text tokens with non-text features\n",
        "    encoded_ds_row = {\n",
        "        \"input_ids\": text_tokens[\"input_ids\"],\n",
        "        \"token_type_ids\": text_tokens[\"token_type_ids\"],\n",
        "        \"attention_mask\": text_tokens[\"attention_mask\"],\n",
        "    }\n",
        "    if stance:\n",
        "        encoded_ds_row.update(\n",
        "            {\n",
        "                \"Stance\": torch.tensor(\n",
        "                    ds_row[\"Stance\"], dtype=torch.float\n",
        "                ),  ### Assuming 'Stance' is represented as 0 or 1\n",
        "            }\n",
        "        )\n",
        "\n",
        "    encoded_ds_row[\"labels\"] = add_labels(ds_row, level_3_cat)\n",
        "\n",
        "    return encoded_ds_row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8vqLU-htg4L"
      },
      "outputs": [],
      "source": [
        "### Training of the model\n",
        "def train_model(train_dl, model, optimizer, class_weights, use_stance=False):\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    num_samples = 0\n",
        "\n",
        "    ### activate dropout, batch norm\n",
        "    model.train()\n",
        "\n",
        "    ### initialize progress bar\n",
        "    batches = tq.tqdm(\n",
        "        enumerate(train_dl), total=len(train_dl), leave=True, colour=\"steelblue\"\n",
        "    )\n",
        "\n",
        "    for batch_idx, data in batches:\n",
        "        ids = data[\"input_ids\"].to(device, dtype=torch.long)\n",
        "        mask = data[\"attention_mask\"].to(device, dtype=torch.long)\n",
        "        token_type_ids = data[\"token_type_ids\"].to(device, dtype=torch.long)\n",
        "        labels = data[\"labels\"].to(device, dtype=torch.float)\n",
        "        if use_stance:\n",
        "            stance = data[\"Stance\"].to(device, dtype=torch.float)\n",
        "            outputs = model(ids, token_type_ids, mask, stance)  ### Forward\n",
        "        else:\n",
        "            outputs = model(ids, token_type_ids, mask)  ### Forward\n",
        "\n",
        "        loss = loss_fn(outputs, labels, class_weights)\n",
        "        losses.append(loss.cpu().detach().numpy())\n",
        "\n",
        "        ### apply thresh 0.5\n",
        "        outputs = torch.sigmoid(outputs).cpu().detach().numpy().round()\n",
        "        labels = labels.cpu().detach().numpy()\n",
        "        correct_predictions += np.sum(outputs == labels)\n",
        "        num_samples += labels.size\n",
        "\n",
        "        ### Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        ### Grad descent step\n",
        "        optimizer.step()\n",
        "\n",
        "        ### Update progress bar\n",
        "        batches.set_description(f\"\")\n",
        "        batches.set_postfix(batch_loss=loss)\n",
        "\n",
        "    accuracy = float(correct_predictions) / num_samples\n",
        "    return model, accuracy, losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-D3Db5Cmup0"
      },
      "source": [
        "# Task 3: Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb-MA5PhyjRl"
      },
      "source": [
        "### Baseline Metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zT5yo4hSyjRl"
      },
      "outputs": [],
      "source": [
        "def f1_baseline(prediction, labels, data):\n",
        "    ### Evaluate F1 overall\n",
        "    f1_overall = f1_score(\n",
        "        y_true=data[labels], y_pred=prediction, average=\"macro\", zero_division=np.nan\n",
        "    )\n",
        "\n",
        "    ### Evaluate F1 per category\n",
        "    f1_per_cat = [\n",
        "        f1_score(y_true=data[cat], y_pred=prediction[:, i])\n",
        "        for i, cat in enumerate(labels)\n",
        "    ]\n",
        "\n",
        "    return f1_overall, f1_per_cat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl_eZIvPtg4O"
      },
      "source": [
        "### Bert base model metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Y_8XN25tg4O"
      },
      "outputs": [],
      "source": [
        "def eval_model(validation_dl, model, class_weights, use_stance=False):\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    num_samples = 0\n",
        "    num_categories = next(iter(validation_dl))[\"labels\"].shape[1]\n",
        "\n",
        "    ### accumulate data over each batch to compute the f1\n",
        "    true_positives = np.array([0 for _ in range(num_categories)])\n",
        "    false_positives = np.array([0 for _ in range(num_categories)])\n",
        "    false_negatives = np.array([0 for _ in range(num_categories)])\n",
        "\n",
        "    ### turn off dropout, fix batch norm\n",
        "    model.eval()\n",
        "\n",
        "    ### show progress bar\n",
        "    batches = tq.tqdm(\n",
        "        enumerate(validation_dl),\n",
        "        total=len(validation_dl),\n",
        "        leave=True,\n",
        "        colour=\"steelblue\",\n",
        "    )\n",
        "    # batches = enumerate(validation_dl)\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, data in batches:\n",
        "            ids = data[\"input_ids\"].to(device, dtype=torch.long)\n",
        "            mask = data[\"attention_mask\"].to(device, dtype=torch.long)\n",
        "            token_type_ids = data[\"token_type_ids\"].to(device, dtype=torch.long)\n",
        "            labels = data[\"labels\"].to(device, dtype=torch.float)\n",
        "            if use_stance:\n",
        "                stance = data[\"Stance\"].to(device, dtype=torch.float)\n",
        "                outputs = model(ids, token_type_ids, mask, stance)  ### Forward\n",
        "            else:\n",
        "                outputs = model(ids, token_type_ids, mask)\n",
        "\n",
        "            loss = loss_fn(outputs, labels, class_weights)\n",
        "            losses.append(loss.cpu().detach().numpy())\n",
        "\n",
        "            ### validation accuracy\n",
        "            ### training sigmoid is in BCEWithLogitsLoss\n",
        "            outputs = torch.sigmoid(outputs).cpu().detach().numpy().round()\n",
        "            labels = labels.cpu().detach().numpy()\n",
        "            correct_predictions += np.sum(outputs == labels)\n",
        "            num_samples += labels.size\n",
        "\n",
        "            ### TP: predicttion == 1, true label == 1\n",
        "            true_positives += np.array(\n",
        "                [\n",
        "                    np.sum(np.logical_and(outputs[:, i] == 1, labels[:, i] == 1))\n",
        "                    for i in range(num_categories)\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            ### FP: prediction == 1, true label == 0\n",
        "            false_positives += np.array(\n",
        "                [\n",
        "                    np.sum(np.logical_and(outputs[:, i] == 1, labels[:, i] == 0))\n",
        "                    for i in range(num_categories)\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            ### FN: prediction == 0, true label == 1\n",
        "            false_negatives += np.array(\n",
        "                [\n",
        "                    np.sum(np.logical_and(outputs[:, i] == 0, labels[:, i] == 1))\n",
        "                    for i in range(num_categories)\n",
        "                ]\n",
        "            )\n",
        "\n",
        "        accuracy = float(correct_predictions) / num_samples\n",
        "        precision = true_positives / (true_positives + false_positives)\n",
        "        recall = true_positives / (true_positives + false_negatives)\n",
        "        f1_per_cat = 2 * (precision * recall) / (precision + recall)\n",
        "        f1_overall = np.mean(f1_per_cat)\n",
        "    return accuracy, losses, f1_overall, f1_per_cat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_item(item, name):\n",
        "    with open(f\"results/{name}.pickle\", \"wb\") as bf:\n",
        "        pickle.dump(item, bf)\n",
        "\n",
        "\n",
        "def read_item(name):\n",
        "    item = None\n",
        "    with open(f\"results/{name}.pickle\", \"rb\") as bf:\n",
        "        item = pickle.load(bf)\n",
        "    return item"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z66N7n0MyjRn"
      },
      "source": [
        "# TASK 4 - Training and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unwGnI1jyjRo"
      },
      "source": [
        "## Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2MIxvmZyjRo"
      },
      "outputs": [],
      "source": [
        "### Random uniform classifier\n",
        "\n",
        "seeds = [333, 666, 999]\n",
        "history_list_uniform = []\n",
        "for seed in seeds:\n",
        "    model_uniform = baseline_model(\n",
        "        \"uniform\", level_3_cat, train_df, columns_to_keep, seed\n",
        "    )\n",
        "\n",
        "    prediction_uniform = np.array(\n",
        "        [clf.predict(X=validation_df[columns_to_keep[1:]]) for clf in model_uniform]\n",
        "    ).T\n",
        "\n",
        "    f1_overall, f1_percat = f1_baseline(\n",
        "        prediction_uniform, labels=level_3_cat, data=validation_df\n",
        "    )\n",
        "\n",
        "    history_list_uniform.append([f1_overall, f1_percat])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_Ige-CkyjRo"
      },
      "outputs": [],
      "source": [
        "save_item(history_list_uniform, \"baseline_uniform\")\n",
        "print(\"Random uniform classifier:\")\n",
        "for h, s in zip(history_list_uniform, seeds):\n",
        "    print(f\"SEED = {s}\")\n",
        "    print(f\"f1_overall = {h[0]}  f1_per_cat = {h[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qutqKTSayjRo"
      },
      "outputs": [],
      "source": [
        "### Majority classifier\n",
        "\n",
        "prediction_majority = baseline_model(\n",
        "    \"most_frequent\", level_3_cat, train_df, columns_to_keep\n",
        ")\n",
        "\n",
        "prediction_majority = np.array(\n",
        "    [clf.predict(X=validation_df[columns_to_keep[1:]]) for clf in model_uniform]\n",
        ").T\n",
        "\n",
        "f1_overall, f1_percat = f1_baseline(\n",
        "    prediction_majority, labels=level_3_cat, data=validation_df\n",
        ")\n",
        "\n",
        "history_majority = (f1_overall, f1_percat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXmHaC7JyjRp"
      },
      "outputs": [],
      "source": [
        "save_item(history_majority, \"baseline_majority\")\n",
        "print(\"Majority classifier:\")\n",
        "print(f\"SEED = {s}\")\n",
        "print(f\"f1_overall = {history_majority[0]}  f1_per_cat = {history_majority[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8fdIGTUyjRp"
      },
      "source": [
        "## Bert-base models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwMmtBcLesDC"
      },
      "outputs": [],
      "source": [
        "def create_data_loaders(tokenized_datasets, batch_size):\n",
        "    train_dl = torch.utils.data.DataLoader(\n",
        "        tokenized_datasets[\"train\"],\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=0,\n",
        "    )\n",
        "\n",
        "    validation_dl = torch.utils.data.DataLoader(\n",
        "        tokenized_datasets[\"validation\"],\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "    )\n",
        "\n",
        "    test_dl = torch.utils.data.DataLoader(\n",
        "        tokenized_datasets[\"test\"],\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "    )\n",
        "    return train_dl, validation_dl, test_dl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PNnK8s9yjRp"
      },
      "outputs": [],
      "source": [
        "def setup(\n",
        "    datasets,\n",
        "    learning_rate,\n",
        "    batch_size=32,\n",
        "    weight_decay=0.01,\n",
        "    premise=False,\n",
        "    stance=False,\n",
        "    ### from here on there is no need to specify the arguments\n",
        "    tokenization_function=tokenize,\n",
        "    tokenizer=tokenizer,\n",
        "    model_class=BERTClass,\n",
        "):\n",
        "    ### tokenize each ds in the datasets dictionary\n",
        "    ### mapping the tokenization function on each dataset\n",
        "    tokenized_datasets = {\n",
        "        split: ds.map(\n",
        "            function=tokenization_function,\n",
        "            fn_kwargs={\"tokenizer\": tokenizer, \"premise\": premise, \"stance\": stance},\n",
        "            batched=True,\n",
        "            remove_columns=[\n",
        "                \"Argument ID\",\n",
        "                \"Conclusion\",\n",
        "                \"Stance\",\n",
        "                \"Premise\",\n",
        "                \"Openness_to_change\",\n",
        "                \"Self_enhancement\",\n",
        "                \"Conversation\",\n",
        "                \"Self_transcendence\",\n",
        "            ],\n",
        "        )\n",
        "        for split, ds in datasets.items()\n",
        "    }\n",
        "\n",
        "    for ds in tokenized_datasets.values():\n",
        "        ds.set_format(type=\"torch\")\n",
        "\n",
        "    train_dl, validation_dl, test_dl = create_data_loaders(\n",
        "        tokenized_datasets, batch_size\n",
        "    )\n",
        "\n",
        "    ### define the model\n",
        "    model = model_class(stance=stance)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    ### define the optimizer\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    return (train_dl, validation_dl, test_dl), model, optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dzbsc1ouesDC"
      },
      "outputs": [],
      "source": [
        "def train_eval(\n",
        "    dls,\n",
        "    model,\n",
        "    optimizer,\n",
        "    class_weights,\n",
        "    n_epochs=1,\n",
        "    save_name=\"0\",\n",
        "    use_stance=False,\n",
        "    ### from here on there is no need to specify the arguments\n",
        "    train_model_f=train_model,\n",
        "    eval_model_f=eval_model,\n",
        "):\n",
        "    model_folder = Path.cwd().joinpath(\"models\")\n",
        "    if not model_folder.exists():\n",
        "        model_folder.mkdir(parents=True)\n",
        "\n",
        "    history = {}\n",
        "    best_f1 = 0\n",
        "    train_dl, validation_dl, test_dl = dls\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        print(f\"Epoch {epoch}/{n_epochs}\")\n",
        "        model, train_acc, train_losses = train_model_f(\n",
        "            train_dl, model, optimizer, class_weights, use_stance\n",
        "        )\n",
        "        val_acc, val_losses, f1_overall, f1_per_cat = eval_model_f(\n",
        "            validation_dl, model, class_weights, use_stance\n",
        "        )\n",
        "\n",
        "        print(\n",
        "            f\"train_loss={np.mean(train_losses):.4f}, val_loss={np.mean(val_losses):.4f}, \",\n",
        "            f\"train_acc={train_acc:.4f}, val_acc={val_acc:.4f}, \",\n",
        "            f\"val_f1_overall={f1_overall:.4f}, \" f\"val_f1_per_cat={f1_per_cat}\",\n",
        "        )\n",
        "\n",
        "        history.update({\"train_acc\": train_acc})\n",
        "        history.update({\"train_losses\": train_losses})\n",
        "        history.update({\"val_acc\": val_acc})\n",
        "        history.update({\"val_losses\": val_losses})\n",
        "        history.update({\"f1_overall\": f1_overall})\n",
        "        history.update({\"f1_per_cat\": f1_per_cat})\n",
        "\n",
        "        ### save the best model\n",
        "        if f1_overall > best_f1:\n",
        "            torch.save(\n",
        "                model.state_dict(),\n",
        "                Path.joinpath(model_folder, f\"model_{save_name}.bin\"),\n",
        "            )\n",
        "            best_f1 = f1_overall\n",
        "\n",
        "    return (history[\"f1_overall\"], history[\"f1_per_cat\"], history[\"train_losses\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZDIZjUUyjRp"
      },
      "outputs": [],
      "source": [
        "### Generic Parameters\n",
        "BATCH_SIZE = 32\n",
        "N_EPOCHS = 1\n",
        "LEARNING_RATE_3 = 3e-5\n",
        "LEARNING_RATE_2 = 2e-5\n",
        "WEIGHT_DECAY = 0.01\n",
        "\n",
        "seeds = [333, 666, 999]\n",
        "# seeds = [333, 666]\n",
        "# class_weights = compute_class_weights(train_df, level_3_cat)\n",
        "class_weights = compute_class_weights_root(train_df, level_3_cat)\n",
        "\n",
        "plot_loss = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbo3ou23yjRp"
      },
      "source": [
        "### Bert Conclusion-Only  1 epoch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1AzDslbesDE"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE_Co = LEARNING_RATE_3\n",
        "\n",
        "### loop over seeds:\n",
        "history_list_c_lr3 = []\n",
        "for seed_idx, seed in enumerate(seeds):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    dls, model_c, optimizer_c = setup(\n",
        "        datasets=datasets,\n",
        "        learning_rate=LEARNING_RATE_Co,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "        premise=False,\n",
        "        stance=False,\n",
        "    )\n",
        "\n",
        "    history = train_eval(\n",
        "        dls=dls,\n",
        "        model=model_c,\n",
        "        optimizer=optimizer_c,\n",
        "        class_weights=class_weights,\n",
        "        n_epochs=N_EPOCHS,\n",
        "        save_name=f\"conclusion_{seed_idx}\",\n",
        "        use_stance=False,\n",
        "    )\n",
        "    history_list_c_lr3.append(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKeFNYKsesDE"
      },
      "outputs": [],
      "source": [
        "if plot_loss:\n",
        "    plt.plot(history_list_c_lr3[0][2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LEARNING_RATE_Co = LEARNING_RATE_2\n",
        "\n",
        "### loop over seeds:\n",
        "history_list_c_lr2 = []\n",
        "for seed_idx, seed in enumerate(seeds):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    dls, model_c, optimizer_c = setup(\n",
        "        datasets=datasets,\n",
        "        learning_rate=LEARNING_RATE_Co,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "        premise=False,\n",
        "        stance=False,\n",
        "    )\n",
        "\n",
        "    history = train_eval(\n",
        "        dls=dls,\n",
        "        model=model_c,\n",
        "        optimizer=optimizer_c,\n",
        "        class_weights=class_weights,\n",
        "        n_epochs=N_EPOCHS,\n",
        "        save_name=f\"conclusion_{seed_idx}\",\n",
        "        use_stance=False,\n",
        "    )\n",
        "    history_list_c_lr2.append(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if plot_loss:\n",
        "    plt.plot(history_list_c_lr2[0][2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ2DbWX_yjRq"
      },
      "source": [
        "### Bert with Conclusion and Premise 1 epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44wx3fjQesDF"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE_CP = LEARNING_RATE_3\n",
        "\n",
        "### loop over seeds:\n",
        "history_list_cp_lr3 = []\n",
        "for seed_idx, seed in enumerate(seeds):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    dls, model_cp, optimizer_cp = setup(\n",
        "        datasets=datasets,\n",
        "        learning_rate=LEARNING_RATE_CP,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "        premise=True,\n",
        "        stance=False,\n",
        "    )\n",
        "\n",
        "    history = train_eval(\n",
        "        dls=dls,\n",
        "        model=model_cp,\n",
        "        optimizer=optimizer_cp,\n",
        "        class_weights=class_weights,\n",
        "        n_epochs=N_EPOCHS,\n",
        "        save_name=f\"conclusion_premise_{seed_idx}\",\n",
        "        use_stance=False,\n",
        "    )\n",
        "    history_list_cp_lr3.append(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yGT6vPxesDG"
      },
      "outputs": [],
      "source": [
        "if plot_loss:\n",
        "    plt.plot(history_list_cp_lr3[0][2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LEARNING_RATE_CP = LEARNING_RATE_2\n",
        "\n",
        "### loop over seeds:\n",
        "history_list_cp_lr2 = []\n",
        "for seed_idx, seed in enumerate(seeds):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    dls, model_cp, optimizer_cp = setup(\n",
        "        datasets=datasets,\n",
        "        learning_rate=LEARNING_RATE_CP,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "        premise=True,\n",
        "        stance=False,\n",
        "    )\n",
        "\n",
        "    history = train_eval(\n",
        "        dls=dls,\n",
        "        model=model_cp,\n",
        "        optimizer=optimizer_cp,\n",
        "        class_weights=class_weights,\n",
        "        n_epochs=N_EPOCHS,\n",
        "        save_name=f\"conclusion_premise_{seed_idx}\",\n",
        "        use_stance=False,\n",
        "    )\n",
        "    history_list_cp_lr2.append(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if plot_loss:\n",
        "    plt.plot(history_list_cp_lr2[0][2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtiW-NujyjRr"
      },
      "source": [
        "### Bert with Conclusion Premise and Stance 1 epoch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDjt5nRAesDG"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE_CPS = LEARNING_RATE_3\n",
        "\n",
        "### loop over seeds:\n",
        "history_list_cps_lr3 = []\n",
        "for seed_idx, seed in enumerate(seeds):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    dls, model_cps, optimizer_cps = setup(\n",
        "        datasets=datasets,\n",
        "        learning_rate=LEARNING_RATE_CPS,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "        premise=True,\n",
        "        stance=True,\n",
        "    )\n",
        "\n",
        "    history = train_eval(\n",
        "        dls=dls,\n",
        "        model=model_cps,\n",
        "        optimizer=optimizer_cps,\n",
        "        class_weights=class_weights,\n",
        "        n_epochs=N_EPOCHS,\n",
        "        save_name=f\"conclusion_premise_stance{seed_idx}\",\n",
        "        use_stance=True,\n",
        "    )\n",
        "    history_list_cps_lr3.append(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ua7iNqpbesDG"
      },
      "outputs": [],
      "source": [
        "if plot_loss:\n",
        "    plt.plot(history_list_cps_lr3[0][2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LEARNING_RATE_CPS = LEARNING_RATE_2\n",
        "\n",
        "### loop over seeds:\n",
        "history_list_cps_lr2 = []\n",
        "for seed_idx, seed in enumerate(seeds):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    dls, model_cps, optimizer_cps = setup(\n",
        "        datasets=datasets,\n",
        "        learning_rate=LEARNING_RATE_CPS,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "        premise=True,\n",
        "        stance=True,\n",
        "    )\n",
        "\n",
        "    history = train_eval(\n",
        "        dls=dls,\n",
        "        model=model_cps,\n",
        "        optimizer=optimizer_cps,\n",
        "        class_weights=class_weights,\n",
        "        n_epochs=N_EPOCHS,\n",
        "        save_name=f\"conclusion_premise_stance{seed_idx}\",\n",
        "        use_stance=True,\n",
        "    )\n",
        "    history_list_cps_lr2.append(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if plot_loss:\n",
        "    plt.plot(history_list_cps_lr2[0][2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2JBkCJkesDH"
      },
      "outputs": [],
      "source": [
        "print(\"Results for 1 epoch of training\")\n",
        "for hl, name in zip(\n",
        "    (\n",
        "        history_list_c_lr3,\n",
        "        history_list_c_lr2,\n",
        "        history_list_cp_lr3,\n",
        "        history_list_cp_lr2,\n",
        "        history_list_cps_lr3,\n",
        "        history_list_cps_lr2,\n",
        "    ),\n",
        "    (\"C_lr3\", \"C_lr2\", \"CP_lr3\", \"CP_lr2\", \"CPS_lr3\", \"CPS_lr2\"),\n",
        "):\n",
        "    save_item(hl, name)\n",
        "    print(name)\n",
        "    for h, s in zip(hl, seeds):\n",
        "        print(f\"SEED = {s}\")\n",
        "        print(f\"F1 overall = {h[0]:.4f}\")\n",
        "        print(f\"F1 per cat = {[ f'{i:.4f}' for i in h[1] ]}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmiyKQsOS9DI"
      },
      "source": [
        "### Bert Conclusion-Only  2 epochs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iF5EcxDFSswk"
      },
      "outputs": [],
      "source": [
        "### loop over seeds:\n",
        "history_list_c_2e = []\n",
        "for seed_idx, seed in enumerate(seeds):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    dls, model_c, optimizer_c = setup(\n",
        "        datasets=datasets,\n",
        "        learning_rate=LEARNING_RATE_Co / 2,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "        premise=False,\n",
        "        stance=False,\n",
        "    )\n",
        "\n",
        "    history = train_eval(\n",
        "        dls=dls,\n",
        "        model=model_c,\n",
        "        optimizer=optimizer_c,\n",
        "        class_weights=class_weights,\n",
        "        n_epochs=N_EPOCHS * 2,\n",
        "        save_name=f\"conclusion_{seed_idx}\",\n",
        "        use_stance=False,\n",
        "    )\n",
        "    history_list_c_2e.append(history)\n",
        "plt.plot(history_list_c_2e[0][2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttPVPV5DTIfI"
      },
      "source": [
        "### Bert with Conclusion and Premise 2 epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70Ow6g_fTW7W"
      },
      "outputs": [],
      "source": [
        "### loop over seeds:\n",
        "history_list_cp_2e = []\n",
        "for seed_idx, seed in enumerate(seeds):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    dls, model_cp, optimizer_cp = setup(\n",
        "        datasets=datasets,\n",
        "        learning_rate=LEARNING_RATE_CP / 2,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "        premise=True,\n",
        "        stance=False,\n",
        "    )\n",
        "\n",
        "    history = train_eval(\n",
        "        dls=dls,\n",
        "        model=model_cp,\n",
        "        optimizer=optimizer_cp,\n",
        "        class_weights=class_weights,\n",
        "        n_epochs=N_EPOCHS * 2,\n",
        "        save_name=f\"conclusion_premise_{seed_idx}\",\n",
        "        use_stance=False,\n",
        "    )\n",
        "    history_list_cp_2e.append(history)\n",
        "plt.plot(history_list_cp_2e[0][2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umNrgBzcTnQz"
      },
      "source": [
        "### Bert with Conclusion Premise and Stance 2 epoch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mneHE29sTmvd"
      },
      "outputs": [],
      "source": [
        "### loop over seeds:\n",
        "history_list_cps_2e = []\n",
        "for seed_idx, seed in enumerate(seeds):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    dls, model_cps, optimizer_cps = setup(\n",
        "        datasets=datasets,\n",
        "        learning_rate=LEARNING_RATE_CPS / 2,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "        premise=True,\n",
        "        stance=True,\n",
        "    )\n",
        "\n",
        "    history = train_eval(\n",
        "        dls=dls,\n",
        "        model=model_cps,\n",
        "        optimizer=optimizer_cps,\n",
        "        class_weights=class_weights,\n",
        "        n_epochs=N_EPOCHS * 2,\n",
        "        save_name=f\"conclusion_premise_stance{seed_idx}\",\n",
        "        use_stance=True,\n",
        "    )\n",
        "    history_list_cps_2e.append(history)\n",
        "plt.plot(history_list_cps_2e[0][2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Results for 2 epochs of training\")\n",
        "for hl, name in zip(\n",
        "    (\n",
        "        history_list_c_2e,\n",
        "        history_list_cp_2e,\n",
        "        history_list_cps_2e,\n",
        "    ),\n",
        "    (\"C_2e\", \"CP_2e\", \"CPS_2e\"),\n",
        "):\n",
        "    save_item(hl, name)\n",
        "    print(name)\n",
        "    for h, s in zip(hl, seeds):\n",
        "        print(f\"SEED = {s}\")\n",
        "        print(f\"F1 overall = {h[0]:.4f}\")\n",
        "        print(f\"F1 per cat = {[ f'{i:.4f}' for i in h[1] ]}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 5: Error Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def avg_hist(hists):\n",
        "    overall = np.average([hist[0] for hist in hists])\n",
        "    per_cat = [np.average([hist[1][i] for hist in hists]) for i in range(4)]\n",
        "    return [overall] + per_cat\n",
        "\n",
        "\n",
        "def std_hist(hists):\n",
        "    overall = np.std([hist[0] for hist in hists])\n",
        "    per_cat = [np.std([hist[1][i] for hist in hists]) for i in range(4)]\n",
        "    return [overall] + per_cat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#####################################\n",
        "\n",
        "### TODO remove: FAKE DATA\n",
        "# fake_hist = [0.6, [0.5, 0.5, 0.7, 0.7], \"fake loss\"]\n",
        "#\n",
        "# history_list_c_lr3 = [fake_hist, fake_hist, fake_hist]\n",
        "# history_list_cp_lr3 = [fake_hist, fake_hist, fake_hist]\n",
        "# history_list_cps_lr3 = [fake_hist, fake_hist, fake_hist]\n",
        "#\n",
        "# history_list_c_lr2 = [fake_hist, fake_hist, fake_hist]\n",
        "# history_list_cp_lr2 = [fake_hist, fake_hist, fake_hist]\n",
        "# history_list_cps_lr2 = [fake_hist, fake_hist, fake_hist]\n",
        "#\n",
        "# history_list_c_2e = [fake_hist, fake_hist, fake_hist]\n",
        "# history_list_cp_2e = [fake_hist, fake_hist, fake_hist]\n",
        "# history_list_cps_2e = [fake_hist, fake_hist, fake_hist]\n",
        "\n",
        "#####################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "READ_DATA = True\n",
        "\n",
        "if READ_DATA:\n",
        "    history_list_c_lr3 = read_item(\"C_lr3\")\n",
        "    history_list_cp_lr3 = read_item(\"CP_lr3\")\n",
        "    history_list_cps_lr3 = read_item(\"CPS_lr3\")\n",
        "\n",
        "    history_list_c_lr2 = read_item(\"C_lr2\")\n",
        "    history_list_cp_lr2 = read_item(\"CP_lr2\")\n",
        "    history_list_cps_lr2 = read_item(\"CPS_lr2\")\n",
        "\n",
        "    history_list_c_2e = read_item(\"C_2e\")\n",
        "    history_list_cp_2e = read_item(\"CP_2e\")\n",
        "    history_list_cps_2e = read_item(\"CPS_2e\")\n",
        "\n",
        "    history_list_uniform = read_item(\"baseline_uniform\")\n",
        "    history_majority = read_item(\"baseline_majority\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### class frequencies of the train set:\n",
        "labels_array = train_df[level_3_cat].to_numpy()\n",
        "class_frequencies = (\n",
        "    np.sum(labels_array, axis=0, dtype=np.single) / labels_array.shape[0]\n",
        ")\n",
        "class_frequencies = np.concatenate([np.zeros(1), class_frequencies], axis=None)\n",
        "\n",
        "### Compute avg, std over the random seeds\n",
        "avg_f1_c_lr3 = avg_hist(history_list_c_lr3)\n",
        "avg_f1_cp_lr3 = avg_hist(history_list_cp_lr3)\n",
        "avg_f1_cps_lr3 = avg_hist(history_list_cps_lr3)\n",
        "\n",
        "avg_f1_c_lr2 = avg_hist(history_list_c_lr2)\n",
        "avg_f1_cp_lr2 = avg_hist(history_list_cp_lr2)\n",
        "avg_f1_cps_lr2 = avg_hist(history_list_cps_lr2)\n",
        "\n",
        "avg_f1_uniform = avg_hist(history_list_uniform)\n",
        "avg_f1_majority = [history_majority[0]] + history_majority[1]\n",
        "\n",
        "avg_f1_c_2e = avg_hist(history_list_c_2e)\n",
        "avg_f1_cp_2e = avg_hist(history_list_cp_2e)\n",
        "avg_f1_cps_2e = avg_hist(history_list_cps_2e)\n",
        "\n",
        "### TODO use this\n",
        "std_f1_c_lr3 = std_hist(history_list_c_lr3)\n",
        "std_f1_cp_lr3 = std_hist(history_list_cp_lr3)\n",
        "std_f1_cps_lr3 = std_hist(history_list_cps_lr3)\n",
        "\n",
        "std_f1_c_lr2 = std_hist(history_list_c_lr2)\n",
        "std_f1_cp_lr2 = std_hist(history_list_cp_lr2)\n",
        "std_f1_cps_lr2 = std_hist(history_list_cps_lr2)\n",
        "\n",
        "std_f1_c_2e = std_hist(history_list_c_2e)\n",
        "std_f1_cp_2e = std_hist(history_list_cp_2e)\n",
        "std_f1_cps_2e = std_hist(history_list_cps_2e)\n",
        "\n",
        "#####################\n",
        "### TODO REMOVE FAKE DATA\n",
        "# std_f1_c_lr3 = [i for i in range(5)]\n",
        "# std_f1_cp_lr3 = [i for i in range(5, 10)]\n",
        "# std_f1_cps_lr3 = [i for i in range(10, 15)]\n",
        "\n",
        "# std_f1_c_lr2 = [i + 1 for i in range(5)]\n",
        "# std_f1_cp_lr2 = [i + 1 for i in range(5, 10)]\n",
        "# std_f1_cps_lr2 = [i + 1 for i in range(10, 15)]\n",
        "\n",
        "# std_f1_c_2e = [i + 1 for i in range(5)]\n",
        "# std_f1_cp_2e = [i + 1 for i in range(5, 10)]\n",
        "# std_f1_cps_2e = [i + 1 for i in range(10, 15)]\n",
        "##############################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "title_font = {\n",
        "    \"size\": 16,\n",
        "    \"weight\": \"bold\",\n",
        "}\n",
        "axis_font = {\n",
        "    \"size\": 11,\n",
        "}\n",
        "legend_title_font = {\"weight\": \"bold\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### STD Versione lunga con 1 e 2 epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Plot: variance over random seeds for Bert Models\n",
        "# 1 grafico per lr (eventualmente mergiare 2 grafici)\n",
        "# 5 colonne (macro + per cat)\n",
        "# punti = std dev (C, CP, CPS)\n",
        "\n",
        "std_devs = (\n",
        "    std_f1_c_lr3,\n",
        "    std_f1_c_2e,\n",
        "    std_f1_cp_lr3,\n",
        "    std_f1_cp_2e,\n",
        "    std_f1_cps_lr3,\n",
        "    std_f1_cps_2e,\n",
        ")\n",
        "color_list = [\"red\", \"red\", \"blue\", \"blue\", \"green\", \"green\"]\n",
        "marker_list = [\"^\", \"s\", \"^\", \"s\", \"^\", \"s\"]\n",
        "label_list = [\n",
        "    \"C_nE=1\",\n",
        "    \"C_nE=2\",\n",
        "    \"CP_nE=1\",\n",
        "    \"CP_nE=2\",\n",
        "    \"CPS_nE=1\",\n",
        "    \"CPS_nE=2\",\n",
        "]\n",
        "\n",
        "x_labels = [\"Macro\", \"C1\", \"C2\", \"C3\", \"C4\"]\n",
        "plt.title(\"STD over different random seeds for Bert models\", fontdict=title_font)\n",
        "plt.xlabel(\"Category\", fontdict=axis_font)\n",
        "plt.ylabel(\"Standard deviation\", fontdict=axis_font)\n",
        "plt.xticks(ticks=range(len(std_devs[0])), labels=x_labels)\n",
        "# plt.yscale(\"log\")\n",
        "\n",
        "for s, color, label, marker in zip(std_devs, color_list, label_list, marker_list):\n",
        "    plt.scatter(x=range(len(s)), y=s, c=color, label=label, marker=marker)\n",
        "plt.legend(\n",
        "    title=\"Models\",\n",
        "    facecolor=(0.9, 0.9, 0.9),\n",
        "    title_fontproperties=legend_title_font,\n",
        "    ncols=3,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### STD versione lunga (con 2 learning rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Plot: variance over random seeds for Bert Models\n",
        "# 1 grafico per lr (eventualmente mergiare 2 grafici)\n",
        "# 5 colonne (macro + per cat)\n",
        "# punti = std dev (C, CP, CPS)\n",
        "\n",
        "std_devs = (\n",
        "    std_f1_c_lr3,\n",
        "    std_f1_c_lr2,\n",
        "    std_f1_cp_lr3,\n",
        "    std_f1_cp_lr2,\n",
        "    std_f1_cps_lr3,\n",
        "    std_f1_cps_lr2,\n",
        ")\n",
        "color_list = [\"red\", \"red\", \"blue\", \"blue\", \"green\", \"green\"]\n",
        "marker_list = [\"^\", \"s\", \"^\", \"s\", \"^\", \"s\"]\n",
        "label_list = [\n",
        "    \"C_lr=3e-5\",\n",
        "    \"C_lr=2e-5\",\n",
        "    \"CP_lr=3e-5\",\n",
        "    \"CP_lr=2e-5\",\n",
        "    \"CPS_lr=3e-5\",\n",
        "    \"CPS_lr=2e-5\",\n",
        "]\n",
        "\n",
        "x_labels = [\"Macro\", \"C1\", \"C2\", \"C3\", \"C4\"]\n",
        "plt.title(\"STD over different random seeds for Bert models\", fontdict=title_font)\n",
        "plt.xlabel(\"Category\", fontdict=axis_font)\n",
        "plt.ylabel(\"Standard deviation\", fontdict=axis_font)\n",
        "plt.xticks(ticks=range(len(std_devs[0])), labels=x_labels)\n",
        "# plt.yscale(\"log\")\n",
        "\n",
        "for s, color, label, marker in zip(std_devs, color_list, label_list, marker_list):\n",
        "    plt.scatter(x=range(len(s)), y=s, c=color, label=label, marker=marker)\n",
        "plt.legend(\n",
        "    title=\"Models\",\n",
        "    facecolor=(0.9, 0.9, 0.9),\n",
        "    title_fontproperties=legend_title_font,\n",
        "    ncols=3,\n",
        "    # loc = \"lower right\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### STD Versione corta con lr3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Plot: variance over random seeds for Bert Models\n",
        "# 1 grafico per lr (eventualmente mergiare 2 grafici)\n",
        "# 5 colonne (macro + per cat)\n",
        "# punti = std dev (C, CP, CPS)\n",
        "\n",
        "### allungabile con altre versioni (learning rate o 2 epoch)\n",
        "std_devs = (\n",
        "    std_f1_c_lr3,\n",
        "    std_f1_cp_lr3,\n",
        "    std_f1_cps_lr3,\n",
        ")\n",
        "color_list = [\"red\", \"blue\", \"green\"]\n",
        "marker_list = [\"o\", \"^\", \"s\"]\n",
        "label_list = [\n",
        "    \"C_lr=3e-5\",\n",
        "    \"CP_lr=3e-5\",\n",
        "    \"CPS_lr=3e-5\",\n",
        "]\n",
        "\n",
        "x_labels = [\"Macro\", \"C1\", \"C2\", \"C3\", \"C4\"]\n",
        "plt.title(\"STD over different random seeds for Bert models\", fontdict=title_font)\n",
        "plt.xlabel(\"Category\", fontdict=axis_font)\n",
        "plt.ylabel(\"Standard deviation\", fontdict=axis_font)\n",
        "plt.xticks(ticks=range(len(std_devs[0])), labels=x_labels)\n",
        "# plt.yscale(\"log\")\n",
        "\n",
        "for s, color, label, marker in zip(std_devs, color_list, label_list, marker_list):\n",
        "    plt.scatter(x=range(len(s)), y=s, c=color, label=label, marker=marker)\n",
        "plt.legend(\n",
        "    title=\"Models\",\n",
        "    facecolor=(0.9, 0.9, 0.9),\n",
        "    title_fontproperties=legend_title_font,\n",
        "    # ncols=3,\n",
        "    loc=\"lower center\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### STD Versione corta con lr2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Plot: variance over random seeds for Bert Models\n",
        "# 1 grafico per lr (eventualmente mergiare 2 grafici)\n",
        "# 5 colonne (macro + per cat)\n",
        "# punti = std dev (C, CP, CPS)\n",
        "\n",
        "### allungabile con altre versioni (learning rate o 2 epoch)\n",
        "std_devs = (\n",
        "    std_f1_c_lr2,\n",
        "    std_f1_cp_lr2,\n",
        "    std_f1_cps_lr2,\n",
        ")\n",
        "color_list = [\"red\", \"blue\", \"green\"]\n",
        "marker_list = [\"o\", \"^\", \"s\"]\n",
        "label_list = [\n",
        "    \"C_lr=2e-5\",\n",
        "    \"CP_lr=2e-5\",\n",
        "    \"CPS_lr=2e-5\",\n",
        "]\n",
        "\n",
        "x_labels = [\"Macro\", \"C1\", \"C2\", \"C3\", \"C4\"]\n",
        "plt.title(\"STD over different random seeds for Bert models\", fontdict=title_font)\n",
        "plt.xlabel(\"Category\", fontdict=axis_font)\n",
        "plt.ylabel(\"Standard deviation\", fontdict=axis_font)\n",
        "plt.xticks(ticks=range(len(std_devs[0])), labels=x_labels)\n",
        "# plt.yscale(\"log\")\n",
        "\n",
        "for s, color, label, marker in zip(std_devs, color_list, label_list, marker_list):\n",
        "    plt.scatter(x=range(len(s)), y=s, c=color, label=label, marker=marker)\n",
        "plt.legend(\n",
        "    title=\"Models\",\n",
        "    facecolor=(0.9, 0.9, 0.9),\n",
        "    title_fontproperties=legend_title_font,\n",
        "    # ncols=3,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### f1_score: baseline e lr=3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Plot: f1 score bert vs baseline: overall\n",
        "# Y: avg f1 score\n",
        "# X: macro + categories (5)\n",
        "# series: models: baseline, (c cp, cps)*2 lr *2epoch\n",
        "\n",
        "\n",
        "f1s = (avg_f1_majority, avg_f1_uniform, avg_f1_c_lr3, avg_f1_cp_lr3, avg_f1_cps_lr3)\n",
        "color_list = [\"orange\", \"red\", \"green\", \"blue\", \"aqua\"]\n",
        "marker_list = [\"v\", \"^\", \"s\", \"o\", \"D\"]\n",
        "label_list = [\n",
        "    \"baseline majority\",\n",
        "    \"baseline uniform\",\n",
        "    \"C_lr=3e-5\",\n",
        "    \"CP_lr=3e-5\",\n",
        "    \"CPS_lr=3e-5\",\n",
        "]\n",
        "\n",
        "x_labels = [\"Macro\", \"C1\", \"C2\", \"C3\", \"C4\"]\n",
        "plt.title(\"Avg f1 score of baseline and Bert models\", fontdict=title_font)\n",
        "plt.xlabel(\"Category\", fontdict=axis_font)\n",
        "plt.ylabel(\"Standard deviation\", fontdict=axis_font)\n",
        "plt.xticks(ticks=range(len(std_devs[0])), labels=x_labels)\n",
        "# plt.yscale(\"log\")\n",
        "\n",
        "for s, color, label, marker in zip(f1s, color_list, label_list, marker_list):\n",
        "    plt.scatter(x=range(len(s)), y=s, c=color, label=label, marker=marker)\n",
        "plt.legend(\n",
        "    title=\"Models\",\n",
        "    facecolor=(0.9, 0.9, 0.9),\n",
        "    title_fontproperties=legend_title_font,\n",
        "    # ncols=3,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### f1_score: baseline e lr=2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Plot: f1 score bert vs baseline: overall\n",
        "# Y: avg f1 score\n",
        "# X: macro + categories (5)\n",
        "# series: models: baseline, (c cp, cps)*2 lr *2epoch\n",
        "\n",
        "\n",
        "f1s = (avg_f1_majority, avg_f1_uniform, avg_f1_c_lr2, avg_f1_cp_lr2, avg_f1_cps_lr2)\n",
        "color_list = [\"orange\", \"red\", \"green\", \"blue\", \"aqua\"]\n",
        "marker_list = [\"v\", \"^\", \"s\", \"o\", \"D\"]\n",
        "label_list = [\n",
        "    \"baseline majority\",\n",
        "    \"baseline uniform\",\n",
        "    \"C_lr=2e-5\",\n",
        "    \"CP_lr=2e-5\",\n",
        "    \"CPS_lr=2e-5\",\n",
        "]\n",
        "\n",
        "x_labels = [\"Macro\", \"C1\", \"C2\", \"C3\", \"C4\"]\n",
        "plt.title(\"Avg f1 score of baseline and Bert models\", fontdict=title_font)\n",
        "plt.xlabel(\"Category\", fontdict=axis_font)\n",
        "plt.ylabel(\"Standard deviation\", fontdict=axis_font)\n",
        "plt.xticks(ticks=range(len(std_devs[0])), labels=x_labels)\n",
        "# plt.yscale(\"log\")\n",
        "\n",
        "for s, color, label, marker in zip(f1s, color_list, label_list, marker_list):\n",
        "    plt.scatter(x=range(len(s)), y=s, c=color, label=label, marker=marker)\n",
        "plt.legend(\n",
        "    title=\"Models\",\n",
        "    facecolor=(0.9, 0.9, 0.9),\n",
        "    title_fontproperties=legend_title_font,\n",
        "    # ncols=3,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### f1 score: versione lunga: baseline + lr2 + lr3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Plot: f1 score bert vs baseline: overall\n",
        "# Y: avg f1 score\n",
        "# X: macro + categories (5)\n",
        "# series: models: baseline, (c cp, cps)*2 lr *2epoch\n",
        "\n",
        "\n",
        "f1s = (\n",
        "    avg_f1_majority,\n",
        "    avg_f1_uniform,\n",
        "    avg_f1_c_lr3,\n",
        "    avg_f1_c_lr2,\n",
        "    avg_f1_cp_lr3,\n",
        "    avg_f1_cp_lr2,\n",
        "    avg_f1_cps_lr3,\n",
        "    avg_f1_cps_lr2,\n",
        ")\n",
        "color_list = [\"red\", \"red\", \"green\", \"green\", \"blue\", \"blue\", \"aqua\", \"aqua\"]\n",
        "marker_list = [\"v\", \"^\", \"s\", \"D\", \"+\", \"x\", \"1\", \"*\"]\n",
        "label_list = [\n",
        "    \"baseline majority\",\n",
        "    \"baseline uniform\",\n",
        "    \"C_lr=3e-5\",\n",
        "    \"C_lr=2e-5\",\n",
        "    \"CP_lr=3e-5\",\n",
        "    \"CP_lr=2e-5\",\n",
        "    \"CPS_lr=3e-5\",\n",
        "    \"CPS_lr=2e-5\",\n",
        "]\n",
        "\n",
        "x_labels = [\"Macro\", \"C1\", \"C2\", \"C3\", \"C4\"]\n",
        "plt.title(\"Avg f1 score of baseline and Bert models\", fontdict=title_font)\n",
        "plt.xlabel(\"Category\", fontdict=axis_font)\n",
        "plt.ylabel(\"Standard deviation\", fontdict=axis_font)\n",
        "plt.xticks(ticks=range(len(std_devs[0])), labels=x_labels)\n",
        "# plt.yscale(\"log\")\n",
        "\n",
        "for s, color, label, marker in zip(f1s, color_list, label_list, marker_list):\n",
        "    plt.scatter(x=range(len(s)), y=s, c=color, label=label, marker=marker)\n",
        "plt.legend(\n",
        "    title=\"Models\",\n",
        "    facecolor=(0.9, 0.9, 0.9),\n",
        "    title_fontproperties=legend_title_font,\n",
        "    # ncols=3,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### f1 score e class frequency, baseline e lr3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Plot: class frequency(column) vs f1 score per model (scatter)\n",
        "# Y: avg f1 score\n",
        "# X: macro + categories (5)\n",
        "# series: models: baseline, (c cp, cps), columns: class frequency\n",
        "\n",
        "\n",
        "f1s = (avg_f1_majority, avg_f1_uniform, avg_f1_c_lr3, avg_f1_cp_lr3, avg_f1_cps_lr3)\n",
        "color_list = [\"orange\", \"red\", \"green\", \"blue\", \"aqua\"]\n",
        "marker_list = [\"v\", \"^\", \"s\", \"o\", \"D\"]\n",
        "label_list = [\n",
        "    \"baseline majority\",\n",
        "    \"baseline uniform\",\n",
        "    \"C_lr=3e-5\",\n",
        "    \"CP_lr=3e-5\",\n",
        "    \"CPS_lr=3e-5\",\n",
        "]\n",
        "\n",
        "x_labels = [\"Macro\", \"C1\", \"C2\", \"C3\", \"C4\"]\n",
        "plt.title(\"Avg f1 score of baseline and Bert models\", fontdict=title_font)\n",
        "plt.xlabel(\"Category\", fontdict=axis_font)\n",
        "plt.ylabel(\"Standard deviation\", fontdict=axis_font)\n",
        "plt.xticks(ticks=range(len(std_devs[0])), labels=x_labels)\n",
        "# plt.yscale(\"log\")\n",
        "\n",
        "for s, color, label, marker in zip(f1s, color_list, label_list, marker_list):\n",
        "    plt.scatter(x=range(len(s)), y=s, c=color, label=label, marker=marker)\n",
        "\n",
        "### TODO plot hist with class freq\n",
        "plt.bar(\n",
        "    x=np.arange(len(class_frequencies)),\n",
        "    height=(class_frequencies),\n",
        "    width=0.4,\n",
        "    color=(1, 1, 0),\n",
        "    edgecolor=\"black\",\n",
        "    label=\"class frequency\",\n",
        "    alpha=0.2,\n",
        ")\n",
        "\n",
        "\n",
        "plt.legend(\n",
        "    title=\"Models\",\n",
        "    facecolor=(0.9, 0.9, 0.9),\n",
        "    title_fontproperties=legend_title_font,\n",
        "    loc=\"lower right\"\n",
        "    # ncols=3,\n",
        "    # draggable=True, ### does not work on vscode\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "_gYGvjafmupz",
        "pa8-cRcgyjRj",
        "N-zxok9DyjRk"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
