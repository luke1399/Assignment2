{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBky0WRKnLDP"
      },
      "outputs": [],
      "source": [
        "%pip install datasets -q\n",
        "%pip install accelerate -U -q\n",
        "%pip install transformers -q\n",
        "%pip install scikit-learn==1.3.2 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLQEVUNkmupt"
      },
      "outputs": [],
      "source": [
        "import urllib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "from transformers import (\n",
        "    BertModel,\n",
        "    BertTokenizer,\n",
        ")\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm.notebook as tq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LITvRFBXmupv"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device: %s\" % device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XD5WRYKmupv"
      },
      "source": [
        "## TASK 1: Corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3HAVDQ9mupw"
      },
      "outputs": [],
      "source": [
        "### Argument urls\n",
        "argument_urls = {\n",
        "    \"train\": \"https://zenodo.org/records/8248658/files/arguments-training.tsv?download=1\",\n",
        "    \"validation\": \"https://zenodo.org/records/8248658/files/arguments-validation.tsv?download=1\",\n",
        "    \"test\": \"https://zenodo.org/records/8248658/files/arguments-test.tsv?download=1\",\n",
        "}\n",
        "\n",
        "### Human values urls\n",
        "level2_values_urls = {\n",
        "    \"train\": \"https://zenodo.org/records/8248658/files/labels-training.tsv?download=1\",\n",
        "    \"validation\": \"https://zenodo.org/records/8248658/files/labels-validation.tsv?download=1\",\n",
        "    \"test\": \"https://zenodo.org/records/8248658/files/labels-test.tsv?download=1\",\n",
        "}\n",
        "### Check that the splits in the dict keys were not misspelled\n",
        "for split in level2_values_urls.keys():\n",
        "    assert split in argument_urls.keys(), \"url dictionary keys misspelled\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbPdz0AMyjRZ"
      },
      "outputs": [],
      "source": [
        "### Create Data folder\n",
        "data_folder = Path.cwd().joinpath(\"Data\")\n",
        "if not data_folder.exists():\n",
        "    data_folder.mkdir(parents=True)\n",
        "\n",
        "### Path for each file\n",
        "argument_paths = {\n",
        "    split: data_folder.joinpath(f\"arguments_{split}.tsv\")\n",
        "    for split in argument_urls.keys()\n",
        "}\n",
        "level2_values_paths = {\n",
        "    split: data_folder.joinpath(f\"labels_{split}.tsv\")\n",
        "    for split in level2_values_urls.keys()\n",
        "}\n",
        "\n",
        "### Download each file if it's not already there\n",
        "for file, path in argument_paths.items():\n",
        "    if not path.exists():\n",
        "        urllib.request.urlretrieve(argument_urls[file], filename=path)\n",
        "for file, path in level2_values_paths.items():\n",
        "    if not path.exists():\n",
        "        urllib.request.urlretrieve(level2_values_urls[file], filename=path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtSpCbncmupw"
      },
      "outputs": [],
      "source": [
        "### Read arguments file (split = train/validation/test)\n",
        "argument_dfs = {\n",
        "    split: pd.read_csv(path, sep=\"\\t\") for split, path in argument_paths.items()\n",
        "}\n",
        "\n",
        "### Read human values file (split = train/validation/test)\n",
        "level2_values_dfs = {\n",
        "    values_split: pd.read_csv(path, sep=\"\\t\")\n",
        "    for values_split, path in level2_values_paths.items()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJDNc3hQyjRb"
      },
      "outputs": [],
      "source": [
        "### Merge arguments and (labels) level 2 values (split = train/validation/test)\n",
        "args_level2vals_dfs = {\n",
        "    split: pd.merge(argument, level2_values_dfs[split], on=\"Argument ID\")\n",
        "    for split, argument in argument_dfs.items()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVuY57e6yjRd"
      },
      "outputs": [],
      "source": [
        "level2_values_dfs[\"train\"].head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvRAFeEiesCz"
      },
      "outputs": [],
      "source": [
        "argument_dfs[\"train\"].head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FJvn74Bmupw"
      },
      "outputs": [],
      "source": [
        "args_level2vals_dfs[\"train\"].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0z1YWKtAmupw"
      },
      "outputs": [],
      "source": [
        "args_level2vals_dfs[\"train\"].head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txRosokSyjRf"
      },
      "outputs": [],
      "source": [
        "### Considering category ranges (0,3),(3,7),(7,13),(13,19)\n",
        "### adding +4, considering the first 4 columns which are not categories\n",
        "level3_categories_ranges = {\n",
        "    \"Openness_to_change\": (4, 7),\n",
        "    \"Self_enhancement\": (7, 11),\n",
        "    \"Conversation\": (11, 17),\n",
        "    \"Self_transcendence\": (17, 23),\n",
        "}\n",
        "columns_to_keep = [\"Argument ID\", \"Conclusion\", \"Stance\", \"Premise\"]\n",
        "level_3_cat = list(level3_categories_ranges.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQ0eequuyjRg"
      },
      "outputs": [],
      "source": [
        "### Creating final dataframes\n",
        "train, validation, test = args_level2vals_dfs.keys()\n",
        "assert train == \"train\" and validation == \"validation\" and test == \"test\"\n",
        "\n",
        "### nm = not merged\n",
        "train_df_nm = args_level2vals_dfs[\"train\"]\n",
        "validation_df_nm = args_level2vals_dfs[\"validation\"]\n",
        "test_df_nm = args_level2vals_dfs[\"test\"]\n",
        "\n",
        "### Creating final dataframes\n",
        "train_df = pd.DataFrame()\n",
        "validation_df = pd.DataFrame()\n",
        "test_df = pd.DataFrame()\n",
        "\n",
        "### Merge lvl2 to lvl 3 (any = OR)\n",
        "for cat, (start, end) in level3_categories_ranges.items():\n",
        "    train_df[cat] = train_df_nm.iloc[:, start:end].any(axis=1)\n",
        "    validation_df[cat] = validation_df_nm.iloc[:, start:end].any(axis=1)\n",
        "    test_df[cat] = test_df_nm.iloc[:, start:end].any(axis=1)\n",
        "\n",
        "### Adding the columns to keep of the original dfs\n",
        "train_df = pd.concat([train_df_nm[columns_to_keep], train_df], axis=1)\n",
        "validation_df = pd.concat([validation_df_nm[columns_to_keep], validation_df], axis=1)\n",
        "test_df = pd.concat([test_df_nm[columns_to_keep], test_df], axis=1)\n",
        "\n",
        "### Define a mapping for \"Stance\" column\n",
        "stance_mapping = {\"in favor of\": 1, \"against\": 0}\n",
        "\n",
        "### Apply the mapping to convert strings to boolean values\n",
        "train_df[\"Stance\"] = train_df[\"Stance\"].map(stance_mapping)\n",
        "validation_df[\"Stance\"] = validation_df[\"Stance\"].map(stance_mapping)\n",
        "test_df[\"Stance\"] = test_df[\"Stance\"].map(stance_mapping)\n",
        "\n",
        "dfs = {\"train\": train_df, \"validation\": validation_df, \"test\": test_df}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVahwSKQyjRg"
      },
      "outputs": [],
      "source": [
        "train_df[\"Conversation\"].sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5NsJaBIesC2"
      },
      "outputs": [],
      "source": [
        "train_df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbzBp0BLesC2"
      },
      "outputs": [],
      "source": [
        "train_df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdC_EHS0mupx"
      },
      "source": [
        "# TASK 2: Model definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xtHCuBamupx"
      },
      "source": [
        "## Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1nlQvZ9yjRh"
      },
      "outputs": [],
      "source": [
        "def baseline_model(strategy, level_3_cat, train_df, columns_to_keep):\n",
        "    clf_list = [DummyClassifier(strategy=strategy) for _ in level_3_cat]\n",
        "    [\n",
        "        clf.fit(X=train_df[columns_to_keep[1:]], y=train_df[cat])\n",
        "        for clf, cat in zip(clf_list, level_3_cat)\n",
        "    ]\n",
        "    return clf_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3dtkYMRmupy"
      },
      "source": [
        "## Bert - base Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1Y_4AZ2mupy"
      },
      "outputs": [],
      "source": [
        "### Convert dataframes into datasets\n",
        "datasets = {split: Dataset.from_pandas(df) for split, df in dfs.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fl7AnlkByjRh"
      },
      "outputs": [],
      "source": [
        "def compute_class_weights(df, cat_labels):\n",
        "    labels_array = df[cat_labels].to_numpy()\n",
        "    n_ones = np.sum(labels_array, axis=0, dtype=np.single)\n",
        "    weights = np.empty_like(n_ones)\n",
        "    n_zeroes = np.array([labels_array.shape[0] - o for o in n_ones])\n",
        "\n",
        "    for class_num, (ones, zeroes) in enumerate(zip(n_ones, n_zeroes)):\n",
        "        weights[class_num] = zeroes / (ones + 1e-4)\n",
        "\n",
        "    print(f\"weigts = {weights}\")\n",
        "    return torch.as_tensor(weights, dtype=torch.float).to(device)\n",
        "\n",
        "\n",
        "def compute_class_weights_root(df, cat_labels):\n",
        "    labels_array = df[cat_labels].to_numpy()\n",
        "    n_ones = np.sum(labels_array, axis=0, dtype=np.single)\n",
        "    weights = np.empty_like(n_ones)\n",
        "    n_zeroes = np.array([labels_array.shape[0] - o for o in n_ones])\n",
        "\n",
        "    for class_num, ones in enumerate(n_ones):\n",
        "        weights[class_num] = np.sqrt(labels_array.shape[0] / (ones + 1e-4))\n",
        "\n",
        "    print(f\"weigts = {weights}\")\n",
        "    return torch.as_tensor(weights, dtype=torch.float).to(device)\n",
        "\n",
        "\n",
        "def loss_fn(outputs, targets, pos_weight=None):\n",
        "    return torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)(outputs, targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zgsc-qEeyjRi"
      },
      "outputs": [],
      "source": [
        "def add_labels(ds_row, labels):\n",
        "    labels_batch = {k: ds_row[k] for k in ds_row.keys() if k in labels}\n",
        "    labels_matrix = np.zeros((len(ds_row[\"Conclusion\"]), len(labels)))\n",
        "    for i, label in enumerate(labels):\n",
        "        labels_matrix[:, i] = labels_batch[label]\n",
        "    return labels_matrix.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmNPnBnstg4K"
      },
      "source": [
        "### NEW\n",
        "### General Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVg7yRcWtg4K"
      },
      "outputs": [],
      "source": [
        "class BERTClass(torch.nn.Module):\n",
        "    def __init__(self, stance=False):\n",
        "        super().__init__()\n",
        "        self.bert_model = BertModel.from_pretrained(\n",
        "            \"bert-base-uncased\", return_dict=True\n",
        "        )\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        if not stance:\n",
        "            self.linear = torch.nn.Linear(768, len(level_3_cat))\n",
        "        else:\n",
        "            ### 769! there is \"stance\" as another input\n",
        "            self.linear = torch.nn.Linear(769, len(level_3_cat))\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids, attn_mask, stance=None):\n",
        "        output = self.bert_model(\n",
        "            input_ids, attention_mask=attn_mask, token_type_ids=token_type_ids\n",
        "        )\n",
        "        output_dropout = self.dropout(output.pooler_output)\n",
        "\n",
        "        if stance is None:\n",
        "            output_linear = self.linear(output_dropout)\n",
        "        else:\n",
        "            ### concatenate stance\n",
        "            stance = stance.view(stance.shape[0], -1)\n",
        "            output_stack = torch.cat((output_dropout, stance), dim=1)\n",
        "            output_linear = self.linear(output_stack)\n",
        "        return output_linear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KC8IMAftg4K"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_a-V-56otg4K"
      },
      "outputs": [],
      "source": [
        "### Encoding\n",
        "def tokenize(ds_row, tokenizer=tokenizer, premise=False, stance=False):\n",
        "    ### Tokenize text columns\n",
        "    print(f\"PREMISE = {premise}  STANCE = {stance}\")\n",
        "    if not premise:\n",
        "        text_tokens = tokenizer(\n",
        "            ds_row[\"Conclusion\"],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=tokenizer.model_max_length // 2,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "    else:\n",
        "        text_tokens = tokenizer(\n",
        "            ds_row[\"Conclusion\"],\n",
        "            ds_row[\"Premise\"],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=tokenizer.model_max_length // 2,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "    ### Combine text tokens with non-text features\n",
        "    encoded_ds_row = {\n",
        "        \"input_ids\": text_tokens[\"input_ids\"],\n",
        "        \"token_type_ids\": text_tokens[\"token_type_ids\"],\n",
        "        \"attention_mask\": text_tokens[\"attention_mask\"],\n",
        "    }\n",
        "    if stance:\n",
        "        encoded_ds_row.update(\n",
        "            {\n",
        "                \"Stance\": torch.tensor(\n",
        "                    ds_row[\"Stance\"], dtype=torch.float\n",
        "                ),  ### Assuming 'Stance' is represented as 0 or 1\n",
        "            }\n",
        "        )\n",
        "\n",
        "    encoded_ds_row[\"labels\"] = add_labels(ds_row, level_3_cat)\n",
        "\n",
        "    return encoded_ds_row"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21gfTGnrtg4K"
      },
      "source": [
        "tokenized_datasets = {\n",
        "    split: ds.map(\n",
        "        function=tokenize,\n",
        "        fn_kwargs={\"tokenizer\": tokenizer, \"premise\": False, \"stance\": False},\n",
        "        batched=True,\n",
        "        remove_columns=[\n",
        "            \"Argument ID\",\n",
        "            \"Conclusion\",\n",
        "            \"Stance\",\n",
        "            \"Premise\",\n",
        "            \"Openness_to_change\",\n",
        "            \"Self_enhancement\",\n",
        "            \"Conversation\",\n",
        "            \"Self_transcendence\",\n",
        "        ],\n",
        "    )\n",
        "    for split, ds in datasets.items()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8vqLU-htg4L"
      },
      "outputs": [],
      "source": [
        "### Training of the model\n",
        "def train_model(train_dl, model, optimizer, class_weights, use_stance=False):\n",
        "    #model = BERTClass()\n",
        "    #model.to(device)\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    num_samples = 0\n",
        "\n",
        "    ### activate dropout, batch norm\n",
        "    model.train()\n",
        "\n",
        "    ### initialize progress bar\n",
        "    batches = tq.tqdm(\n",
        "        enumerate(train_dl), total=len(train_dl), leave=True, colour=\"steelblue\"\n",
        "    )\n",
        "\n",
        "    for batch_idx, data in batches:\n",
        "        ids = data[\"input_ids\"].to(device, dtype=torch.long)\n",
        "        mask = data[\"attention_mask\"].to(device, dtype=torch.long)\n",
        "        token_type_ids = data[\"token_type_ids\"].to(device, dtype=torch.long)\n",
        "        labels = data[\"labels\"].to(device, dtype=torch.float)\n",
        "        if use_stance:\n",
        "            stance = data[\"Stance\"].to(device, dtype=torch.float)\n",
        "            outputs = model(ids, token_type_ids, mask, stance)  ### Forward\n",
        "        else:\n",
        "            outputs = model(ids, token_type_ids, mask)  ### Forward\n",
        "\n",
        "        loss = loss_fn(outputs, labels, class_weights)\n",
        "        losses.append(loss.cpu().detach().numpy())\n",
        "\n",
        "        ### apply thresh 0.5\n",
        "        outputs = torch.sigmoid(outputs).cpu().detach().numpy().round()\n",
        "        labels = labels.cpu().detach().numpy()\n",
        "        correct_predictions += np.sum(outputs == labels)\n",
        "        num_samples += labels.size\n",
        "\n",
        "        ### Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        ### Grad descent step\n",
        "        optimizer.step()\n",
        "\n",
        "        ### Update progress bar\n",
        "        batches.set_description(f\"\")\n",
        "        batches.set_postfix(batch_loss=loss)\n",
        "\n",
        "    accuracy = float(correct_predictions) / num_samples\n",
        "    return model, accuracy, losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gYGvjafmupz"
      },
      "source": [
        "### Conclusion Only Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGxhrO8pyjRi"
      },
      "source": [
        "class BERTConclusionClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERTConclusionClass, self).__init__()\n",
        "        self.bert_model = BertModel.from_pretrained(\n",
        "            \"bert-base-uncased\", return_dict=True\n",
        "        )\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.linear = torch.nn.Linear(768, len(level_3_cat))\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids, attn_mask):\n",
        "        output = self.bert_model(\n",
        "            input_ids, attention_mask=attn_mask, token_type_ids=token_type_ids\n",
        "        )\n",
        "        output_dropout = self.dropout(output.pooler_output)\n",
        "        output_linear = self.linear(output_dropout)\n",
        "        return output_linear"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uFOBtbHmupz"
      },
      "source": [
        "### Encoding for Conclusion only model\n",
        "def tokenize_c(ds_row, tokenizer):\n",
        "    ### Tokenize text columns\n",
        "    text_tokens = tokenizer(\n",
        "        ds_row[\"Conclusion\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=tokenizer.model_max_length // 2,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "\n",
        "    ### Combine text tokens with non-text features\n",
        "    encoded_ds_row = {\n",
        "        \"input_ids\": text_tokens[\"input_ids\"],\n",
        "        \"token_type_ids\": text_tokens[\"token_type_ids\"],\n",
        "        \"attention_mask\": text_tokens[\"attention_mask\"],\n",
        "    }\n",
        "\n",
        "    encoded_ds_row[\"labels\"] = add_labels(ds_row, level_3_cat)\n",
        "\n",
        "    return encoded_ds_row"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AvhZqjctg4M"
      },
      "source": [
        "tokenized_datasets = {\n",
        "    split: ds.map(\n",
        "        function=tokenize_c,\n",
        "        fn_kwargs={\"tokenizer\": tokenizer},\n",
        "        batched=True,\n",
        "        remove_columns=[\n",
        "            \"Argument ID\",\n",
        "            \"Conclusion\",\n",
        "            \"Stance\",\n",
        "            \"Premise\",\n",
        "            \"Openness_to_change\",\n",
        "            \"Self_enhancement\",\n",
        "            \"Conversation\",\n",
        "            \"Self_transcendence\",\n",
        "        ],\n",
        "    )\n",
        "    for split, ds in datasets.items()\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZwkEWrIyjRj"
      },
      "source": [
        "### Training of the model\n",
        "def train_model_c(train_dl, model, optimizer, class_weights):\n",
        "    model = BERTConclusionClass()\n",
        "    model.to(device)\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    num_samples = 0\n",
        "\n",
        "    ### activate dropout, batch norm\n",
        "    model.train()\n",
        "\n",
        "    ### initialize progress bar\n",
        "    batches = tq.tqdm(\n",
        "        enumerate(train_dl), total=len(train_dl), leave=True, colour=\"steelblue\"\n",
        "    )\n",
        "\n",
        "    for batch_idx, data in batches:\n",
        "        ids = data[\"input_ids\"].to(device, dtype=torch.long)\n",
        "        mask = data[\"attention_mask\"].to(device, dtype=torch.long)\n",
        "        token_type_ids = data[\"token_type_ids\"].to(device, dtype=torch.long)\n",
        "        labels = data[\"labels\"].to(device, dtype=torch.float)\n",
        "\n",
        "        ### Forward\n",
        "        outputs = model(ids, token_type_ids, mask)\n",
        "\n",
        "        loss = loss_fn(outputs, labels, class_weights)\n",
        "        losses.append(loss.cpu().detach().numpy())\n",
        "\n",
        "        ### apply thresh 0.5\n",
        "        outputs = torch.sigmoid(outputs).cpu().detach().numpy().round()\n",
        "        labels = labels.cpu().detach().numpy()\n",
        "        correct_predictions += np.sum(outputs == labels)\n",
        "        num_samples += labels.size\n",
        "\n",
        "        ### Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        ### Grad descent step\n",
        "        optimizer.step()\n",
        "\n",
        "        ### Update progress bar\n",
        "        batches.set_description(f\"\")\n",
        "        batches.set_postfix(batch_loss=loss)\n",
        "\n",
        "    accuracy = float(correct_predictions) / num_samples\n",
        "    return model, accuracy, losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pa8-cRcgyjRj"
      },
      "source": [
        "### Conclusion - Premise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ9VnojzesC7"
      },
      "source": [
        "class BERTConclusionPremiseClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERTConclusionPremiseClass, self).__init__()\n",
        "        self.bert_model = BertModel.from_pretrained(\n",
        "            \"bert-base-uncased\", return_dict=True\n",
        "        )\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.linear = torch.nn.Linear(768, len(level_3_cat))\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids, attn_mask):\n",
        "        output = self.bert_model(\n",
        "            input_ids, attention_mask=attn_mask, token_type_ids=token_type_ids\n",
        "        )\n",
        "        output_dropout = self.dropout(output.pooler_output)\n",
        "        output_linear = self.linear(output_dropout)\n",
        "        return output_linear"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wP4BqwzhyjRk"
      },
      "source": [
        "### Encoding for Conclusion - Premise model\n",
        "def tokenize_cp(ds_row, tokenizer):\n",
        "    ### Tokenize text columns\n",
        "    text_tokens = tokenizer(\n",
        "        ds_row[\"Conclusion\"],\n",
        "        ds_row[\"Premise\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=tokenizer.model_max_length // 2,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "\n",
        "    ### Combine text tokens with non-text features\n",
        "    encoded_ds_row = {\n",
        "        \"input_ids\": text_tokens[\"input_ids\"],\n",
        "        \"token_type_ids\": text_tokens[\"token_type_ids\"],\n",
        "        \"attention_mask\": text_tokens[\"attention_mask\"],\n",
        "    }\n",
        "    encoded_ds_row[\"labels\"] = add_labels(ds_row, level_3_cat)\n",
        "\n",
        "    return encoded_ds_row"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2QDTWJresC8"
      },
      "source": [
        "### Training of the model\n",
        "def train_model_cp(train_dl, model, optimizer, class_weights):\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    num_samples = 0\n",
        "\n",
        "    ### activate droput, batch norm\n",
        "    model.train()\n",
        "\n",
        "    ### initialize progress bar\n",
        "    batches = tq.tqdm(\n",
        "        enumerate(train_dl), total=len(train_dl), leave=True, colour=\"steelblue\"\n",
        "    )\n",
        "    for batch_idx, data in batches:\n",
        "        ids = data[\"input_ids\"].to(device, dtype=torch.long)\n",
        "        mask = data[\"attention_mask\"].to(device, dtype=torch.long)\n",
        "        token_type_ids = data[\"token_type_ids\"].to(device, dtype=torch.long)\n",
        "        labels = data[\"labels\"].to(device, dtype=torch.float)\n",
        "\n",
        "        ### Forward\n",
        "        outputs = model(ids, token_type_ids, mask)\n",
        "        loss = loss_fn(outputs, labels, class_weights)\n",
        "        losses.append(loss.cpu().detach().numpy())\n",
        "\n",
        "        ### thresh 0.5\n",
        "        outputs = torch.sigmoid(outputs).cpu().detach().numpy().round()\n",
        "        labels = labels.cpu().detach().numpy()\n",
        "        correct_predictions += np.sum(outputs == labels)\n",
        "        num_samples += labels.size\n",
        "\n",
        "        ### Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        ### Grad descent step\n",
        "        optimizer.step()\n",
        "\n",
        "        ### Update progress bar\n",
        "        batches.set_description(f\"\")\n",
        "        batches.set_postfix(batch_loss=loss)\n",
        "\n",
        "    accuracy = float(correct_predictions) / num_samples\n",
        "    return model, accuracy, losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-zxok9DyjRk"
      },
      "source": [
        "### Conclusion - Premise - Stance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hft7E9J0esC8"
      },
      "source": [
        "class BERTConclusionPremiseStanceClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERTConclusionPremiseStanceClass, self).__init__()\n",
        "        self.bert_model = BertModel.from_pretrained(\n",
        "            \"bert-base-uncased\", return_dict=True\n",
        "        )\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.linear = torch.nn.Linear(\n",
        "            769, len(level_3_cat)\n",
        "        )  ### 769! there is \"stance\" as another input\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids, attn_mask, stance):\n",
        "        output = self.bert_model(\n",
        "            input_ids, attention_mask=attn_mask, token_type_ids=token_type_ids\n",
        "        )\n",
        "        output_dropout = self.dropout(output.pooler_output)\n",
        "        ### concatenate stance\n",
        "        stance = stance.view(stance.shape[0], -1)\n",
        "        output_stack = torch.cat((output_dropout, stance), dim=1)\n",
        "        output_linear = self.linear(output_stack)\n",
        "        return output_linear"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGUg3Q8_yjRk"
      },
      "source": [
        "### Encoding for Conclusion - Premise - Stance model\n",
        "def tokenize_cps(ds_row, tokenizer):\n",
        "    ### Tokenize text columns\n",
        "    text_tokens = tokenizer(\n",
        "        ds_row[\"Conclusion\"],\n",
        "        ds_row[\"Premise\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=tokenizer.model_max_length // 2,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "\n",
        "    ### Combine text tokens with non-text features\n",
        "    encoded_ds_row = {\n",
        "        \"input_ids\": text_tokens[\"input_ids\"],\n",
        "        \"token_type_ids\": text_tokens[\"token_type_ids\"],\n",
        "        \"attention_mask\": text_tokens[\"attention_mask\"],\n",
        "        \"Stance\": torch.tensor(\n",
        "            ds_row[\"Stance\"], dtype=torch.float\n",
        "        ),  ### Assuming 'Stance' is represented as 0 or 1\n",
        "    }\n",
        "    encoded_ds_row[\"labels\"] = add_labels(ds_row, level_3_cat)\n",
        "\n",
        "    return encoded_ds_row"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0zGGowqesC9"
      },
      "source": [
        "### Training of the model\n",
        "def train_model_cps(train_dl, model, optimizer, class_weights):\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    num_samples = 0\n",
        "\n",
        "    ### activate droput, batch norm\n",
        "    model.train()\n",
        "\n",
        "    ### initialize progress bar\n",
        "    batches = tq.tqdm(\n",
        "        enumerate(train_dl), total=len(train_dl), leave=True, colour=\"steelblue\"\n",
        "    )\n",
        "\n",
        "    for batch_idx, data in batches:\n",
        "        ids = data[\"input_ids\"].to(device, dtype=torch.long)\n",
        "        mask = data[\"attention_mask\"].to(device, dtype=torch.long)\n",
        "        token_type_ids = data[\"token_type_ids\"].to(device, dtype=torch.long)\n",
        "        labels = data[\"labels\"].to(device, dtype=torch.float)\n",
        "        stance = data[\"Stance\"].to(device, dtype=torch.float)\n",
        "\n",
        "        ### Forward\n",
        "        outputs = model(ids, token_type_ids, mask, stance)\n",
        "        loss = loss_fn(outputs, labels, class_weights)\n",
        "        losses.append(loss.cpu().detach().numpy())\n",
        "\n",
        "        ### Training accuracy, apply sigmoid, round (apply thresh 0.5)\n",
        "        outputs = torch.sigmoid(outputs).cpu().detach().numpy().round()\n",
        "        labels = labels.cpu().detach().numpy()\n",
        "        correct_predictions += np.sum(outputs == labels)\n",
        "        num_samples += labels.size\n",
        "\n",
        "        ### Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        ### Grad descent step\n",
        "        optimizer.step()\n",
        "\n",
        "        ### Update progress bar\n",
        "        batches.set_description(f\"\")\n",
        "        batches.set_postfix(batch_loss=loss)\n",
        "\n",
        "    accuracy = float(correct_predictions) / num_samples\n",
        "    return model, accuracy, losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-D3Db5Cmup0"
      },
      "source": [
        "# Task 3: Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb-MA5PhyjRl"
      },
      "source": [
        "### Baseline Metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zT5yo4hSyjRl"
      },
      "outputs": [],
      "source": [
        "def f1_baseline(prediction, labels, data):\n",
        "    ### Evaluate F1 overall\n",
        "    f1_overall = f1_score(\n",
        "        y_true=data[labels], y_pred=prediction, average=\"macro\", zero_division=np.nan\n",
        "    )\n",
        "\n",
        "    ### Evaluate F1 per category\n",
        "    f1_per_cat = [\n",
        "        f1_score(y_true=data[cat], y_pred=prediction[:, i])\n",
        "        for i, cat in enumerate(labels)\n",
        "    ]\n",
        "\n",
        "    return f1_overall, f1_per_cat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl_eZIvPtg4O"
      },
      "source": [
        "### NEW\n",
        "### BERT BASE METRIC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Y_8XN25tg4O"
      },
      "outputs": [],
      "source": [
        "def eval_model(validation_dl, model, class_weights, use_stance=False):\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    num_samples = 0\n",
        "    num_categories = next(iter(validation_dl))[\"labels\"].shape[1]\n",
        "\n",
        "    ### accumulate data over each batch to compute the f1\n",
        "    true_positives = np.array([0 for _ in range(num_categories)])\n",
        "    false_positives = np.array([0 for _ in range(num_categories)])\n",
        "    false_negatives = np.array([0 for _ in range(num_categories)])\n",
        "\n",
        "    ### turn off dropout, fix batch norm\n",
        "    model.eval()\n",
        "\n",
        "    ### show progress bar\n",
        "    batches = tq.tqdm(\n",
        "        enumerate(validation_dl),\n",
        "        total=len(validation_dl),\n",
        "        leave=True,\n",
        "        colour=\"steelblue\",\n",
        "    )\n",
        "    # batches = enumerate(validation_dl)\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, data in batches:\n",
        "            ids = data[\"input_ids\"].to(device, dtype=torch.long)\n",
        "            mask = data[\"attention_mask\"].to(device, dtype=torch.long)\n",
        "            token_type_ids = data[\"token_type_ids\"].to(device, dtype=torch.long)\n",
        "            labels = data[\"labels\"].to(device, dtype=torch.float)\n",
        "            if use_stance:\n",
        "                stance = data[\"Stance\"].to(device, dtype=torch.float)\n",
        "                outputs = model(ids, token_type_ids, mask, stance)  ### Forward\n",
        "            else:\n",
        "                outputs = model(ids, token_type_ids, mask)\n",
        "\n",
        "            loss = loss_fn(outputs, labels, class_weights)\n",
        "            losses.append(loss.cpu().detach().numpy())\n",
        "\n",
        "            ### validation accuracy\n",
        "            ### training sigmoid is in BCEWithLogitsLoss\n",
        "            outputs = torch.sigmoid(outputs).cpu().detach().numpy().round()\n",
        "            labels = labels.cpu().detach().numpy()\n",
        "            correct_predictions += np.sum(outputs == labels)\n",
        "            num_samples += labels.size\n",
        "\n",
        "            ### TP: predicttion == 1, true label == 1\n",
        "            true_positives += np.array(\n",
        "                [\n",
        "                    np.sum(np.logical_and(outputs[:, i] == 1, labels[:, i] == 1))\n",
        "                    for i in range(num_categories)\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            ### FP: prediction == 1, true label == 0\n",
        "            false_positives += np.array(\n",
        "                [\n",
        "                    np.sum(np.logical_and(outputs[:, i] == 1, labels[:, i] == 0))\n",
        "                    for i in range(num_categories)\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            ### FN: prediction == 0, true label == 1\n",
        "            false_negatives += np.array(\n",
        "                [\n",
        "                    np.sum(np.logical_and(outputs[:, i] == 0, labels[:, i] == 1))\n",
        "                    for i in range(num_categories)\n",
        "                ]\n",
        "            )\n",
        "\n",
        "        accuracy = float(correct_predictions) / num_samples\n",
        "        precision = true_positives / (true_positives + false_positives)\n",
        "        recall = true_positives / (true_positives + false_negatives)\n",
        "        f1_per_cat = 2 * (precision * recall) / (precision + recall)\n",
        "        f1_overall = np.mean(f1_per_cat)\n",
        "    return accuracy, losses, f1_overall, f1_per_cat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfIXNdlnyjRm"
      },
      "source": [
        "### Bert Conclusion-Only Model Metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNS6dCDNyjRm"
      },
      "source": [
        "def eval_model_c(validation_dl, model, class_weights):\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    num_samples = 0\n",
        "    num_categories = next(iter(validation_dl))[\"labels\"].shape[1]\n",
        "\n",
        "    ### accumulate data over each batch to compute the f1\n",
        "    true_positives = np.array([0 for _ in range(num_categories)])\n",
        "    false_positives = np.array([0 for _ in range(num_categories)])\n",
        "    false_negatives = np.array([0 for _ in range(num_categories)])\n",
        "\n",
        "    ### set model to eval mode (turn off dropout, fix batch norm)\n",
        "    model.eval()\n",
        "\n",
        "    ### show progress bar\n",
        "    batches = tq.tqdm(\n",
        "        enumerate(validation_dl),\n",
        "        total=len(validation_dl),\n",
        "        leave=True,\n",
        "        colour=\"steelblue\",\n",
        "    )\n",
        "    # batches = enumerate(validation_dl)\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, data in batches:\n",
        "            ids = data[\"input_ids\"].to(device, dtype=torch.long)\n",
        "            mask = data[\"attention_mask\"].to(device, dtype=torch.long)\n",
        "            token_type_ids = data[\"token_type_ids\"].to(device, dtype=torch.long)\n",
        "            labels = data[\"labels\"].to(device, dtype=torch.float)\n",
        "            outputs = model(ids, token_type_ids, mask)\n",
        "\n",
        "            loss = loss_fn(outputs, labels, class_weights)\n",
        "            losses.append(loss.cpu().detach().numpy())\n",
        "\n",
        "            ### validation accuracy\n",
        "\n",
        "            ### training sigmoid is in BCEWithLogitsLoss\n",
        "            outputs = torch.sigmoid(outputs).cpu().detach().numpy().round()\n",
        "            labels = labels.cpu().detach().numpy()\n",
        "            correct_predictions += np.sum(outputs == labels)\n",
        "            num_samples += labels.size\n",
        "\n",
        "            ### TP: predicttion == 1, true label == 1.\n",
        "            true_positives += np.array(\n",
        "                [\n",
        "                    np.sum(np.logical_and(outputs[:, i] == 1, labels[:, i] == 1))\n",
        "                    for i in range(num_categories)\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            ### FP: prediction == 1, true label == 0\n",
        "            false_positives += np.array(\n",
        "                [\n",
        "                    np.sum(np.logical_and(outputs[:, i] == 1, labels[:, i] == 0))\n",
        "                    for i in range(num_categories)\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            ### FN: prediction == 0, true label == 1\n",
        "            false_negatives += np.array(\n",
        "                [\n",
        "                    np.sum(np.logical_and(outputs[:, i] == 0, labels[:, i] == 1))\n",
        "                    for i in range(num_categories)\n",
        "                ]\n",
        "            )\n",
        "\n",
        "        accuracy = float(correct_predictions) / num_samples\n",
        "        precision = true_positives / (true_positives + false_positives)\n",
        "        recall = true_positives / (true_positives + false_negatives)\n",
        "        f1_per_cat = 2 * (precision * recall) / (precision + recall)\n",
        "        f1_overall = np.mean(f1_per_cat)\n",
        "\n",
        "    return accuracy, losses, f1_overall, f1_per_cat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVl10aXpmup1"
      },
      "source": [
        "### Bert Conclusion - Premise Model Metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTZoa56aesC_"
      },
      "source": [
        "def eval_model_cp(validation_dl, model, class_weights):\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    num_samples = 0\n",
        "    num_categories = next(iter(validation_dl))[\"labels\"].shape[1]\n",
        "\n",
        "    ### accumulate data over each batch to compute the f1\n",
        "    true_positives = np.array([0 for _ in range(num_categories)])\n",
        "    false_positives = np.array([0 for _ in range(num_categories)])\n",
        "    false_negatives = np.array([0 for _ in range(num_categories)])\n",
        "\n",
        "    ### turn off dropout, fix batch norm\n",
        "    model.eval()\n",
        "\n",
        "    ### show progress bar\n",
        "    batches = tq.tqdm(\n",
        "        enumerate(validation_dl),\n",
        "        total=len(validation_dl),\n",
        "        leave=True,\n",
        "        colour=\"steelblue\",\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, data in batches:\n",
        "            ids = data[\"input_ids\"].to(device, dtype=torch.long)\n",
        "            mask = data[\"attention_mask\"].to(device, dtype=torch.long)\n",
        "            token_type_ids = data[\"token_type_ids\"].to(device, dtype=torch.long)\n",
        "            labels = data[\"labels\"].to(device, dtype=torch.float)\n",
        "            outputs = model(ids, token_type_ids, mask)\n",
        "\n",
        "            loss = loss_fn(outputs, labels, class_weights)  # maybe remove class weights\n",
        "            losses.append(loss.cpu().detach().numpy())\n",
        "\n",
        "            ### validation accuracy\n",
        "\n",
        "            ### training sigmoid is in BCEWithLogitsLoss\n",
        "            outputs = torch.sigmoid(outputs).cpu().detach().numpy().round()\n",
        "            labels = labels.cpu().detach().numpy()\n",
        "            correct_predictions += np.sum(outputs == labels)\n",
        "            num_samples += labels.size\n",
        "\n",
        "            ### TP: predicttion == 1, true label == 1.\n",
        "            true_positives += np.array(\n",
        "                [\n",
        "                    np.sum(np.logical_and(outputs[:, i] == 1, labels[:, i] == 1))\n",
        "                    for i in range(num_categories)\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            ### FP: prediction == 1, true label == 0\n",
        "            false_positives += np.array(\n",
        "                [\n",
        "                    np.sum(np.logical_and(outputs[:, i] == 1, labels[:, i] == 0))\n",
        "                    for i in range(num_categories)\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            ### FN: prediction == 0, true label == 1\n",
        "            false_negatives += np.array(\n",
        "                [\n",
        "                    np.sum(np.logical_and(outputs[:, i] == 0, labels[:, i] == 1))\n",
        "                    for i in range(num_categories)\n",
        "                ]\n",
        "            )\n",
        "\n",
        "        accuracy = float(correct_predictions) / num_samples\n",
        "        precision = true_positives / (true_positives + false_positives)\n",
        "        recall = true_positives / (true_positives + false_negatives)\n",
        "        f1_per_cat = 2 * (precision * recall) / (precision + recall)\n",
        "        f1_overall = np.mean(f1_per_cat)\n",
        "\n",
        "    return accuracy, losses, f1_overall, f1_per_cat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hciZ0KMYmup1"
      },
      "source": [
        "### Bert Conclusion - Premise - Stance Model Metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17g35UXcesDA"
      },
      "source": [
        "def eval_model_cps(validation_dl, model, class_weights):\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    num_samples = 0\n",
        "    num_categories = next(iter(validation_dl))[\"labels\"].shape[1]\n",
        "\n",
        "    ### accumulate data over each batch to compute the f1\n",
        "    true_positives = np.array([0 for _ in range(num_categories)])\n",
        "    false_positives = np.array([0 for _ in range(num_categories)])\n",
        "    false_negatives = np.array([0 for _ in range(num_categories)])\n",
        "\n",
        "    ### turn off dropout, fix batch norm\n",
        "    model.eval()\n",
        "\n",
        "    ### show progress bar\n",
        "    batches = tq.tqdm(\n",
        "        enumerate(validation_dl),\n",
        "        total=len(validation_dl),\n",
        "        leave=True,\n",
        "        colour=\"steelblue\",\n",
        "    )\n",
        "    # batches = enumerate(validation_dl)\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, data in batches:\n",
        "            ids = data[\"input_ids\"].to(device, dtype=torch.long)\n",
        "            mask = data[\"attention_mask\"].to(device, dtype=torch.long)\n",
        "            token_type_ids = data[\"token_type_ids\"].to(device, dtype=torch.long)\n",
        "            labels = data[\"labels\"].to(device, dtype=torch.float)\n",
        "            stance = data[\"Stance\"].to(device, dtype=torch.float)\n",
        "            outputs = model(ids, token_type_ids, mask, stance)\n",
        "\n",
        "            loss = loss_fn(outputs, labels, class_weights)\n",
        "            losses.append(loss.cpu().detach().numpy())\n",
        "\n",
        "            ### validation accuracy\n",
        "            ### training sigmoid is in BCEWithLogitsLoss\n",
        "            outputs = torch.sigmoid(outputs).cpu().detach().numpy().round()\n",
        "            labels = labels.cpu().detach().numpy()\n",
        "            correct_predictions += np.sum(outputs == labels)\n",
        "            num_samples += labels.size\n",
        "\n",
        "            ### TP: predicttion == 1, true label == 1\n",
        "            true_positives += np.array(\n",
        "                [\n",
        "                    np.sum(np.logical_and(outputs[:, i] == 1, labels[:, i] == 1))\n",
        "                    for i in range(num_categories)\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            ### FP: prediction == 1, true label == 0\n",
        "            false_positives += np.array(\n",
        "                [\n",
        "                    np.sum(np.logical_and(outputs[:, i] == 1, labels[:, i] == 0))\n",
        "                    for i in range(num_categories)\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            ### FN: prediction == 0, true label == 1\n",
        "            false_negatives += np.array(\n",
        "                [\n",
        "                    np.sum(np.logical_and(outputs[:, i] == 0, labels[:, i] == 1))\n",
        "                    for i in range(num_categories)\n",
        "                ]\n",
        "            )\n",
        "\n",
        "        accuracy = float(correct_predictions) / num_samples\n",
        "        precision = true_positives / (true_positives + false_positives)\n",
        "        recall = true_positives / (true_positives + false_negatives)\n",
        "        f1_per_cat = 2 * (precision * recall) / (precision + recall)\n",
        "        f1_overall = np.mean(f1_per_cat)\n",
        "    return accuracy, losses, f1_overall, f1_per_cat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z66N7n0MyjRn"
      },
      "source": [
        "# TASK 4 - Training and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unwGnI1jyjRo"
      },
      "source": [
        "## Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2MIxvmZyjRo"
      },
      "outputs": [],
      "source": [
        "model_uniform = baseline_model(\"uniform\", level_3_cat, train_df, columns_to_keep)\n",
        "prediction_uniform = np.array(\n",
        "    [clf.predict(X=test_df[columns_to_keep[1:]]) for clf in model_uniform]\n",
        ").T\n",
        "f1_overall, f1_percat = f1_baseline(\n",
        "    prediction_uniform, labels=level_3_cat, data=test_df\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_Ige-CkyjRo"
      },
      "outputs": [],
      "source": [
        "print(f\"f1_overall = {f1_overall}  \\t f1 per category: {f1_percat}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qutqKTSayjRo"
      },
      "outputs": [],
      "source": [
        "prediction_majority = baseline_model(\n",
        "    \"most_frequent\", level_3_cat, train_df, columns_to_keep\n",
        ")\n",
        "prediction_uniform = np.array(\n",
        "    [clf.predict(X=test_df[columns_to_keep[1:]]) for clf in model_uniform]\n",
        ").T\n",
        "f1_overall, f1_percat = f1_baseline(\n",
        "    prediction_uniform, labels=level_3_cat, data=test_df\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXmHaC7JyjRp"
      },
      "outputs": [],
      "source": [
        "print(f\"f1_overall = {f1_overall}  \\t f1 per category: {f1_percat}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8fdIGTUyjRp"
      },
      "source": [
        "## Bert-base models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwMmtBcLesDC"
      },
      "outputs": [],
      "source": [
        "def create_data_loaders(tokenized_datasets, batch_size):\n",
        "    train_dl = torch.utils.data.DataLoader(\n",
        "        tokenized_datasets[\"train\"],\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=0,\n",
        "    )\n",
        "\n",
        "    validation_dl = torch.utils.data.DataLoader(\n",
        "        tokenized_datasets[\"validation\"],\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "    )\n",
        "\n",
        "    test_dl = torch.utils.data.DataLoader(\n",
        "        tokenized_datasets[\"test\"],\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "    )\n",
        "    return train_dl, validation_dl, test_dl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PNnK8s9yjRp"
      },
      "outputs": [],
      "source": [
        "def setup(\n",
        "    datasets,\n",
        "    learning_rate,\n",
        "    batch_size=32,\n",
        "    weight_decay=0.01,\n",
        "    premise=False,\n",
        "    stance=False,\n",
        "    ### from here on there is no need to specify the arguments\n",
        "    tokenization_function=tokenize,\n",
        "    tokenizer=tokenizer,\n",
        "    model_class=BERTClass,\n",
        "):\n",
        "    ### tokenize each ds in the datasets dictionary\n",
        "    ### mapping the tokenization function on each dataset\n",
        "    tokenized_datasets = {\n",
        "        split: ds.map(\n",
        "            function=tokenization_function,\n",
        "            fn_kwargs={\"tokenizer\": tokenizer, \"premise\": premise, \"stance\": stance},\n",
        "            batched=True,\n",
        "            remove_columns=[\n",
        "                \"Argument ID\",\n",
        "                \"Conclusion\",\n",
        "                \"Stance\",\n",
        "                \"Premise\",\n",
        "                \"Openness_to_change\",\n",
        "                \"Self_enhancement\",\n",
        "                \"Conversation\",\n",
        "                \"Self_transcendence\",\n",
        "            ],\n",
        "        )\n",
        "        for split, ds in datasets.items()\n",
        "    }\n",
        "\n",
        "    for ds in tokenized_datasets.values():\n",
        "        ds.set_format(type=\"torch\")\n",
        "\n",
        "    train_dl, validation_dl, test_dl = create_data_loaders(\n",
        "        tokenized_datasets, batch_size\n",
        "    )\n",
        "\n",
        "    ### define the model\n",
        "    model = model_class(stance=stance)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    ### define the optimizer\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    return (train_dl, validation_dl, test_dl), model, optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dzbsc1ouesDC"
      },
      "outputs": [],
      "source": [
        "def train_eval(\n",
        "    dls,\n",
        "    model,\n",
        "    optimizer,\n",
        "    class_weights,\n",
        "    n_epochs=1,\n",
        "    save_name=\"0\",\n",
        "    use_stance=False,\n",
        "    ### from here on there is no need to specify the arguments\n",
        "    train_model_f=train_model,\n",
        "    eval_model_f=eval_model,\n",
        "):\n",
        "    model_folder = Path.cwd().joinpath(\"models\")\n",
        "    if not model_folder.exists():\n",
        "        model_folder.mkdir(parents=True)\n",
        "\n",
        "    history = {}\n",
        "    best_f1 = 0\n",
        "    train_dl, validation_dl, test_dl = dls\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        print(f\"Epoch {epoch}/{n_epochs}\")\n",
        "        model, train_acc, train_losses = train_model_f(\n",
        "            train_dl, model, optimizer, class_weights, use_stance\n",
        "        )\n",
        "        val_acc, val_losses, f1_overall, f1_per_cat = eval_model_f(\n",
        "            validation_dl, model, class_weights, use_stance\n",
        "        )\n",
        "\n",
        "        print(\n",
        "            f\"train_loss={np.mean(train_losses):.4f}, val_loss={np.mean(val_losses):.4f}, \",\n",
        "            f\"train_acc={train_acc:.4f}, val_acc={val_acc:.4f}, \",\n",
        "            f\"val_f1_overall={f1_overall:.4f}, \" f\"val_f1_per_cat={f1_per_cat}\",\n",
        "        )\n",
        "\n",
        "        ### TODO return a more meaningful history\n",
        "        history.update({\"train_acc\": train_acc})\n",
        "        history.update({\"train_losses\": train_losses})\n",
        "        history.update({\"val_acc\": val_acc})\n",
        "        history.update({\"val_losses\": val_losses})\n",
        "        history.update({\"f1_overall\": f1_overall})\n",
        "        history.update({\"f1_per_cat\": f1_per_cat})\n",
        "\n",
        "        ### save the best model\n",
        "        if f1_overall > best_f1:\n",
        "            torch.save(\n",
        "                model.state_dict(),\n",
        "                Path.joinpath(model_folder, f\"model_{save_name}.bin\"),\n",
        "            )\n",
        "            best_f1 = f1_overall\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZDIZjUUyjRp"
      },
      "outputs": [],
      "source": [
        "### Generic Parameters\n",
        "BATCH_SIZE = 32\n",
        "N_EPOCHS = 1\n",
        "LEARNING_RATE = 3e-5\n",
        "WEIGHT_DECAY = 0.01\n",
        "\n",
        "# seeds = [333, 666 , 999]\n",
        "seeds = [333, 666]\n",
        "# class_weights = compute_class_weights(train_df, level_3_cat)\n",
        "class_weights = compute_class_weights_root(train_df, level_3_cat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbo3ou23yjRp"
      },
      "source": [
        "### Bert Conclusion-Only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tljHMkuPesDD"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE_Co = LEARNING_RATE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1AzDslbesDE"
      },
      "outputs": [],
      "source": [
        "### loop over seeds:\n",
        "history_list_c = []\n",
        "for seed_idx, seed in enumerate(seeds):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    dls, model_c, optimizer_c = setup(\n",
        "        datasets=datasets,\n",
        "        learning_rate=LEARNING_RATE_Co,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "        premise=False,\n",
        "        stance=False,\n",
        "    )\n",
        "\n",
        "    history = train_eval(\n",
        "        dls=dls,\n",
        "        model=model_c,\n",
        "        optimizer=optimizer_c,\n",
        "        class_weights=class_weights,\n",
        "        n_epochs=N_EPOCHS,\n",
        "        save_name=f\"conclusion_{seed_idx}\",\n",
        "        use_stance=False,\n",
        "    )\n",
        "    history_list_c.append(history)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history_list_c[0][\"train_losses\"])"
      ],
      "metadata": {
        "id": "XpGFBXtAEWc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKeFNYKsesDE"
      },
      "outputs": [],
      "source": [
        "### Plot loss\n",
        "plt.plot(history_list_c[1][\"train_losses\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ2DbWX_yjRq"
      },
      "source": [
        "### Bert with Conclusion and Premise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLyC__oaesDE"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE_CP = LEARNING_RATE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44wx3fjQesDF"
      },
      "outputs": [],
      "source": [
        "### loop over seeds:\n",
        "history_list_cp = []\n",
        "for seed_idx, seed in enumerate(seeds):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    dls, model_cp, optimizer_cp = setup(\n",
        "        datasets=datasets,\n",
        "        learning_rate=LEARNING_RATE_CP,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "        premise=True,\n",
        "        stance=False,\n",
        "    )\n",
        "\n",
        "    history = train_eval(\n",
        "        dls=dls,\n",
        "        model=model_cp,\n",
        "        optimizer=optimizer_cp,\n",
        "        class_weights=class_weights,\n",
        "        n_epochs=N_EPOCHS,\n",
        "        save_name=f\"conclusion_premise_{seed_idx}\",\n",
        "        use_stance=False,\n",
        "    )\n",
        "    history_list_cp.append(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yGT6vPxesDG"
      },
      "outputs": [],
      "source": [
        "### Plot loss\n",
        "plt.plot(history_list_cp[0][\"train_losses\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history_list_cp[1][\"train_losses\"])"
      ],
      "metadata": {
        "id": "kSCKbry4EZK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtiW-NujyjRr"
      },
      "source": [
        "### Bert with Conclusion Premise and Stance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrwbfsczesDG"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE_CPS = LEARNING_RATE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDjt5nRAesDG"
      },
      "outputs": [],
      "source": [
        "### loop over seeds:\n",
        "history_list_cps = []\n",
        "for seed_idx, seed in enumerate(seeds):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    dls, model_cps, optimizer_cps = setup(\n",
        "        datasets=datasets,\n",
        "        learning_rate=LEARNING_RATE_CPS,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "        premise=True,\n",
        "        stance=True,\n",
        "    )\n",
        "\n",
        "    history = train_eval(\n",
        "        dls=dls,\n",
        "        model=model_cps,\n",
        "        optimizer=optimizer_cps,\n",
        "        class_weights=class_weights,\n",
        "        n_epochs=N_EPOCHS,\n",
        "        save_name=f\"conclusion_premise_stance{seed_idx}\",\n",
        "        use_stance=True,\n",
        "    )\n",
        "    history_list_cps.append(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ua7iNqpbesDG"
      },
      "outputs": [],
      "source": [
        "### Plot loss\n",
        "plt.plot(history_list_cps[0][\"train_losses\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history_list_cps[1][\"train_losses\"])"
      ],
      "metadata": {
        "id": "CFKAXSV-EblE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2JBkCJkesDH"
      },
      "outputs": [],
      "source": [
        "for hl, name in zip(\n",
        "    (\n",
        "        history_list_c,\n",
        "        history_list_cp,\n",
        "        history_list_cps,\n",
        "    ),\n",
        "    (\"C\", \"CP\", \"CPS\"),\n",
        "):\n",
        "    print(name)\n",
        "    for h, s in zip(hl, seeds):\n",
        "        print(f\"SEED = {s}\")\n",
        "        print(f\"F1 overall = {h['f1_overall']:.4f}\")\n",
        "        print(f\"F1 per cat = {[ f'{i:.4f}' for i in h['f1_per_cat'] ]}\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "_gYGvjafmupz",
        "pa8-cRcgyjRj",
        "N-zxok9DyjRk"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}